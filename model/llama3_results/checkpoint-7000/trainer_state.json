{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.437691835481891,
  "eval_steps": 500,
  "global_step": 7000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0024554941682013503,
      "grad_norm": 1.890625,
      "learning_rate": 0.0002999926326129666,
      "loss": 2.5624,
      "step": 5
    },
    {
      "epoch": 0.004910988336402701,
      "grad_norm": 1.15625,
      "learning_rate": 0.0002999852652259332,
      "loss": 1.8958,
      "step": 10
    },
    {
      "epoch": 0.007366482504604052,
      "grad_norm": 0.95703125,
      "learning_rate": 0.0002999778978388998,
      "loss": 1.7575,
      "step": 15
    },
    {
      "epoch": 0.009821976672805401,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002999705304518664,
      "loss": 1.7818,
      "step": 20
    },
    {
      "epoch": 0.012277470841006752,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029996316306483295,
      "loss": 1.7395,
      "step": 25
    },
    {
      "epoch": 0.014732965009208104,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0002999557956777996,
      "loss": 1.6751,
      "step": 30
    },
    {
      "epoch": 0.017188459177409455,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0002999484282907662,
      "loss": 1.6777,
      "step": 35
    },
    {
      "epoch": 0.019643953345610803,
      "grad_norm": 0.7578125,
      "learning_rate": 0.00029994106090373276,
      "loss": 1.6903,
      "step": 40
    },
    {
      "epoch": 0.022099447513812154,
      "grad_norm": 0.7578125,
      "learning_rate": 0.00029993369351669937,
      "loss": 1.6558,
      "step": 45
    },
    {
      "epoch": 0.024554941682013505,
      "grad_norm": 0.52734375,
      "learning_rate": 0.00029992632612966597,
      "loss": 1.7068,
      "step": 50
    },
    {
      "epoch": 0.027010435850214856,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0002999189587426326,
      "loss": 1.6797,
      "step": 55
    },
    {
      "epoch": 0.029465930018416207,
      "grad_norm": 0.75,
      "learning_rate": 0.0002999115913555992,
      "loss": 1.6282,
      "step": 60
    },
    {
      "epoch": 0.03192142418661756,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0002999042239685658,
      "loss": 1.6868,
      "step": 65
    },
    {
      "epoch": 0.03437691835481891,
      "grad_norm": 0.78125,
      "learning_rate": 0.0002998968565815324,
      "loss": 1.6463,
      "step": 70
    },
    {
      "epoch": 0.03683241252302026,
      "grad_norm": 0.67578125,
      "learning_rate": 0.000299889489194499,
      "loss": 1.6212,
      "step": 75
    },
    {
      "epoch": 0.039287906691221605,
      "grad_norm": 0.53515625,
      "learning_rate": 0.0002998821218074656,
      "loss": 1.6118,
      "step": 80
    },
    {
      "epoch": 0.041743400859422956,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0002998747544204322,
      "loss": 1.6257,
      "step": 85
    },
    {
      "epoch": 0.04419889502762431,
      "grad_norm": 0.6484375,
      "learning_rate": 0.0002998673870333988,
      "loss": 1.6071,
      "step": 90
    },
    {
      "epoch": 0.04665438919582566,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00029986001964636536,
      "loss": 1.6227,
      "step": 95
    },
    {
      "epoch": 0.04910988336402701,
      "grad_norm": 0.5859375,
      "learning_rate": 0.00029985265225933197,
      "loss": 1.6257,
      "step": 100
    },
    {
      "epoch": 0.05156537753222836,
      "grad_norm": 0.61328125,
      "learning_rate": 0.00029984528487229863,
      "loss": 1.6457,
      "step": 105
    },
    {
      "epoch": 0.05402087170042971,
      "grad_norm": 0.78515625,
      "learning_rate": 0.0002998379174852652,
      "loss": 1.5698,
      "step": 110
    },
    {
      "epoch": 0.056476365868631064,
      "grad_norm": 0.74609375,
      "learning_rate": 0.0002998305500982318,
      "loss": 1.6289,
      "step": 115
    },
    {
      "epoch": 0.058931860036832415,
      "grad_norm": 0.85546875,
      "learning_rate": 0.00029982318271119844,
      "loss": 1.5811,
      "step": 120
    },
    {
      "epoch": 0.061387354205033766,
      "grad_norm": 0.671875,
      "learning_rate": 0.000299815815324165,
      "loss": 1.6014,
      "step": 125
    },
    {
      "epoch": 0.06384284837323512,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0002998084479371316,
      "loss": 1.5838,
      "step": 130
    },
    {
      "epoch": 0.06629834254143646,
      "grad_norm": 0.703125,
      "learning_rate": 0.0002998010805500982,
      "loss": 1.5753,
      "step": 135
    },
    {
      "epoch": 0.06875383670963782,
      "grad_norm": 0.8125,
      "learning_rate": 0.0002997937131630648,
      "loss": 1.6502,
      "step": 140
    },
    {
      "epoch": 0.07120933087783916,
      "grad_norm": 0.7421875,
      "learning_rate": 0.0002997863457760314,
      "loss": 1.5842,
      "step": 145
    },
    {
      "epoch": 0.07366482504604052,
      "grad_norm": 0.75390625,
      "learning_rate": 0.000299778978388998,
      "loss": 1.5313,
      "step": 150
    },
    {
      "epoch": 0.07612031921424187,
      "grad_norm": 0.671875,
      "learning_rate": 0.00029977161100196457,
      "loss": 1.6529,
      "step": 155
    },
    {
      "epoch": 0.07857581338244321,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00029976424361493123,
      "loss": 1.5527,
      "step": 160
    },
    {
      "epoch": 0.08103130755064457,
      "grad_norm": 0.6953125,
      "learning_rate": 0.00029975687622789784,
      "loss": 1.5213,
      "step": 165
    },
    {
      "epoch": 0.08348680171884591,
      "grad_norm": 0.8203125,
      "learning_rate": 0.0002997495088408644,
      "loss": 1.5373,
      "step": 170
    },
    {
      "epoch": 0.08594229588704727,
      "grad_norm": 0.640625,
      "learning_rate": 0.00029974214145383105,
      "loss": 1.5603,
      "step": 175
    },
    {
      "epoch": 0.08839779005524862,
      "grad_norm": 0.7578125,
      "learning_rate": 0.0002997347740667976,
      "loss": 1.5232,
      "step": 180
    },
    {
      "epoch": 0.09085328422344997,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0002997274066797642,
      "loss": 1.5541,
      "step": 185
    },
    {
      "epoch": 0.09330877839165132,
      "grad_norm": 0.76953125,
      "learning_rate": 0.0002997200392927308,
      "loss": 1.5089,
      "step": 190
    },
    {
      "epoch": 0.09576427255985268,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0002997126719056974,
      "loss": 1.4982,
      "step": 195
    },
    {
      "epoch": 0.09821976672805402,
      "grad_norm": 0.6640625,
      "learning_rate": 0.000299705304518664,
      "loss": 1.5387,
      "step": 200
    },
    {
      "epoch": 0.10067526089625538,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0002996979371316306,
      "loss": 1.5322,
      "step": 205
    },
    {
      "epoch": 0.10313075506445672,
      "grad_norm": 0.71484375,
      "learning_rate": 0.00029969056974459723,
      "loss": 1.5491,
      "step": 210
    },
    {
      "epoch": 0.10558624923265807,
      "grad_norm": 0.78125,
      "learning_rate": 0.00029968320235756383,
      "loss": 1.4781,
      "step": 215
    },
    {
      "epoch": 0.10804174340085942,
      "grad_norm": 0.734375,
      "learning_rate": 0.00029967583497053044,
      "loss": 1.4663,
      "step": 220
    },
    {
      "epoch": 0.11049723756906077,
      "grad_norm": 0.7578125,
      "learning_rate": 0.00029966846758349705,
      "loss": 1.4721,
      "step": 225
    },
    {
      "epoch": 0.11295273173726213,
      "grad_norm": 0.80078125,
      "learning_rate": 0.00029966110019646365,
      "loss": 1.4293,
      "step": 230
    },
    {
      "epoch": 0.11540822590546347,
      "grad_norm": 0.79296875,
      "learning_rate": 0.00029965373280943026,
      "loss": 1.534,
      "step": 235
    },
    {
      "epoch": 0.11786372007366483,
      "grad_norm": 0.8046875,
      "learning_rate": 0.0002996463654223968,
      "loss": 1.4751,
      "step": 240
    },
    {
      "epoch": 0.12031921424186617,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0002996389980353634,
      "loss": 1.4776,
      "step": 245
    },
    {
      "epoch": 0.12277470841006753,
      "grad_norm": 0.76953125,
      "learning_rate": 0.00029963163064833007,
      "loss": 1.4635,
      "step": 250
    },
    {
      "epoch": 0.1252302025782689,
      "grad_norm": 0.7734375,
      "learning_rate": 0.0002996242632612966,
      "loss": 1.4522,
      "step": 255
    },
    {
      "epoch": 0.12768569674647023,
      "grad_norm": 0.7890625,
      "learning_rate": 0.00029961689587426323,
      "loss": 1.4212,
      "step": 260
    },
    {
      "epoch": 0.13014119091467158,
      "grad_norm": 0.71484375,
      "learning_rate": 0.00029960952848722983,
      "loss": 1.4773,
      "step": 265
    },
    {
      "epoch": 0.13259668508287292,
      "grad_norm": 0.76953125,
      "learning_rate": 0.00029960216110019644,
      "loss": 1.4628,
      "step": 270
    },
    {
      "epoch": 0.13505217925107427,
      "grad_norm": 0.76171875,
      "learning_rate": 0.00029959479371316304,
      "loss": 1.4899,
      "step": 275
    },
    {
      "epoch": 0.13750767341927564,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00029958742632612965,
      "loss": 1.4784,
      "step": 280
    },
    {
      "epoch": 0.13996316758747698,
      "grad_norm": 0.76953125,
      "learning_rate": 0.00029958005893909625,
      "loss": 1.52,
      "step": 285
    },
    {
      "epoch": 0.14241866175567833,
      "grad_norm": 0.734375,
      "learning_rate": 0.00029957269155206286,
      "loss": 1.449,
      "step": 290
    },
    {
      "epoch": 0.14487415592387967,
      "grad_norm": 0.8046875,
      "learning_rate": 0.00029956532416502946,
      "loss": 1.4756,
      "step": 295
    },
    {
      "epoch": 0.14732965009208104,
      "grad_norm": 0.80078125,
      "learning_rate": 0.000299557956777996,
      "loss": 1.4894,
      "step": 300
    },
    {
      "epoch": 0.1497851442602824,
      "grad_norm": 0.80859375,
      "learning_rate": 0.0002995505893909627,
      "loss": 1.4637,
      "step": 305
    },
    {
      "epoch": 0.15224063842848373,
      "grad_norm": 0.75,
      "learning_rate": 0.0002995432220039292,
      "loss": 1.4979,
      "step": 310
    },
    {
      "epoch": 0.15469613259668508,
      "grad_norm": 0.8984375,
      "learning_rate": 0.00029953585461689583,
      "loss": 1.3443,
      "step": 315
    },
    {
      "epoch": 0.15715162676488642,
      "grad_norm": 0.73828125,
      "learning_rate": 0.00029952848722986244,
      "loss": 1.4225,
      "step": 320
    },
    {
      "epoch": 0.1596071209330878,
      "grad_norm": 0.83203125,
      "learning_rate": 0.00029952111984282904,
      "loss": 1.4105,
      "step": 325
    },
    {
      "epoch": 0.16206261510128914,
      "grad_norm": 0.76171875,
      "learning_rate": 0.00029951375245579565,
      "loss": 1.4273,
      "step": 330
    },
    {
      "epoch": 0.16451810926949048,
      "grad_norm": 0.9296875,
      "learning_rate": 0.00029950638506876225,
      "loss": 1.3653,
      "step": 335
    },
    {
      "epoch": 0.16697360343769183,
      "grad_norm": 0.828125,
      "learning_rate": 0.00029949901768172886,
      "loss": 1.395,
      "step": 340
    },
    {
      "epoch": 0.1694290976058932,
      "grad_norm": 0.79296875,
      "learning_rate": 0.00029949165029469546,
      "loss": 1.4229,
      "step": 345
    },
    {
      "epoch": 0.17188459177409454,
      "grad_norm": 0.79296875,
      "learning_rate": 0.00029948428290766207,
      "loss": 1.4015,
      "step": 350
    },
    {
      "epoch": 0.17434008594229589,
      "grad_norm": 0.79296875,
      "learning_rate": 0.00029947691552062867,
      "loss": 1.355,
      "step": 355
    },
    {
      "epoch": 0.17679558011049723,
      "grad_norm": 0.76953125,
      "learning_rate": 0.0002994695481335953,
      "loss": 1.4237,
      "step": 360
    },
    {
      "epoch": 0.17925107427869857,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0002994621807465619,
      "loss": 1.4044,
      "step": 365
    },
    {
      "epoch": 0.18170656844689995,
      "grad_norm": 0.70703125,
      "learning_rate": 0.00029945481335952843,
      "loss": 1.4071,
      "step": 370
    },
    {
      "epoch": 0.1841620626151013,
      "grad_norm": 0.77734375,
      "learning_rate": 0.00029944744597249504,
      "loss": 1.389,
      "step": 375
    },
    {
      "epoch": 0.18661755678330263,
      "grad_norm": 0.875,
      "learning_rate": 0.0002994400785854617,
      "loss": 1.4321,
      "step": 380
    },
    {
      "epoch": 0.18907305095150398,
      "grad_norm": 0.81640625,
      "learning_rate": 0.00029943271119842825,
      "loss": 1.438,
      "step": 385
    },
    {
      "epoch": 0.19152854511970535,
      "grad_norm": 0.8046875,
      "learning_rate": 0.00029942534381139485,
      "loss": 1.329,
      "step": 390
    },
    {
      "epoch": 0.1939840392879067,
      "grad_norm": 0.91796875,
      "learning_rate": 0.00029941797642436146,
      "loss": 1.3801,
      "step": 395
    },
    {
      "epoch": 0.19643953345610804,
      "grad_norm": 0.94921875,
      "learning_rate": 0.00029941060903732806,
      "loss": 1.4172,
      "step": 400
    },
    {
      "epoch": 0.19889502762430938,
      "grad_norm": 0.8515625,
      "learning_rate": 0.00029940324165029467,
      "loss": 1.4276,
      "step": 405
    },
    {
      "epoch": 0.20135052179251076,
      "grad_norm": 0.859375,
      "learning_rate": 0.0002993958742632613,
      "loss": 1.4853,
      "step": 410
    },
    {
      "epoch": 0.2038060159607121,
      "grad_norm": 0.9609375,
      "learning_rate": 0.0002993885068762279,
      "loss": 1.3291,
      "step": 415
    },
    {
      "epoch": 0.20626151012891344,
      "grad_norm": 0.84375,
      "learning_rate": 0.0002993811394891945,
      "loss": 1.3678,
      "step": 420
    },
    {
      "epoch": 0.2087170042971148,
      "grad_norm": 0.92578125,
      "learning_rate": 0.0002993737721021611,
      "loss": 1.3611,
      "step": 425
    },
    {
      "epoch": 0.21117249846531613,
      "grad_norm": 0.88671875,
      "learning_rate": 0.00029936640471512764,
      "loss": 1.3621,
      "step": 430
    },
    {
      "epoch": 0.2136279926335175,
      "grad_norm": 0.82421875,
      "learning_rate": 0.0002993590373280943,
      "loss": 1.3599,
      "step": 435
    },
    {
      "epoch": 0.21608348680171885,
      "grad_norm": 0.921875,
      "learning_rate": 0.00029935166994106085,
      "loss": 1.3102,
      "step": 440
    },
    {
      "epoch": 0.2185389809699202,
      "grad_norm": 0.8671875,
      "learning_rate": 0.00029934430255402746,
      "loss": 1.3651,
      "step": 445
    },
    {
      "epoch": 0.22099447513812154,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0002993369351669941,
      "loss": 1.2977,
      "step": 450
    },
    {
      "epoch": 0.2234499693063229,
      "grad_norm": 1.1640625,
      "learning_rate": 0.00029932956777996067,
      "loss": 1.3383,
      "step": 455
    },
    {
      "epoch": 0.22590546347452425,
      "grad_norm": 0.828125,
      "learning_rate": 0.0002993222003929273,
      "loss": 1.3716,
      "step": 460
    },
    {
      "epoch": 0.2283609576427256,
      "grad_norm": 0.9609375,
      "learning_rate": 0.0002993148330058939,
      "loss": 1.3649,
      "step": 465
    },
    {
      "epoch": 0.23081645181092694,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0002993074656188605,
      "loss": 1.3428,
      "step": 470
    },
    {
      "epoch": 0.2332719459791283,
      "grad_norm": 0.89453125,
      "learning_rate": 0.0002993000982318271,
      "loss": 1.2922,
      "step": 475
    },
    {
      "epoch": 0.23572744014732966,
      "grad_norm": 0.890625,
      "learning_rate": 0.0002992927308447937,
      "loss": 1.2937,
      "step": 480
    },
    {
      "epoch": 0.238182934315531,
      "grad_norm": 0.9453125,
      "learning_rate": 0.0002992853634577603,
      "loss": 1.3385,
      "step": 485
    },
    {
      "epoch": 0.24063842848373235,
      "grad_norm": 0.91796875,
      "learning_rate": 0.0002992779960707269,
      "loss": 1.293,
      "step": 490
    },
    {
      "epoch": 0.2430939226519337,
      "grad_norm": 0.96484375,
      "learning_rate": 0.0002992706286836935,
      "loss": 1.2884,
      "step": 495
    },
    {
      "epoch": 0.24554941682013506,
      "grad_norm": 0.89453125,
      "learning_rate": 0.00029926326129666006,
      "loss": 1.3132,
      "step": 500
    },
    {
      "epoch": 0.2480049109883364,
      "grad_norm": 0.875,
      "learning_rate": 0.0002992558939096267,
      "loss": 1.2671,
      "step": 505
    },
    {
      "epoch": 0.2504604051565378,
      "grad_norm": 0.9765625,
      "learning_rate": 0.0002992485265225933,
      "loss": 1.2348,
      "step": 510
    },
    {
      "epoch": 0.2529158993247391,
      "grad_norm": 0.921875,
      "learning_rate": 0.0002992411591355599,
      "loss": 1.2352,
      "step": 515
    },
    {
      "epoch": 0.25537139349294047,
      "grad_norm": 0.87890625,
      "learning_rate": 0.0002992337917485265,
      "loss": 1.2665,
      "step": 520
    },
    {
      "epoch": 0.2578268876611418,
      "grad_norm": 0.97265625,
      "learning_rate": 0.0002992264243614931,
      "loss": 1.2768,
      "step": 525
    },
    {
      "epoch": 0.26028238182934316,
      "grad_norm": 0.9765625,
      "learning_rate": 0.0002992190569744597,
      "loss": 1.3342,
      "step": 530
    },
    {
      "epoch": 0.2627378759975445,
      "grad_norm": 0.87109375,
      "learning_rate": 0.0002992116895874263,
      "loss": 1.3049,
      "step": 535
    },
    {
      "epoch": 0.26519337016574585,
      "grad_norm": 0.92578125,
      "learning_rate": 0.0002992043222003929,
      "loss": 1.248,
      "step": 540
    },
    {
      "epoch": 0.2676488643339472,
      "grad_norm": 0.8828125,
      "learning_rate": 0.0002991969548133595,
      "loss": 1.2811,
      "step": 545
    },
    {
      "epoch": 0.27010435850214853,
      "grad_norm": 0.91015625,
      "learning_rate": 0.0002991895874263261,
      "loss": 1.2239,
      "step": 550
    },
    {
      "epoch": 0.27255985267034993,
      "grad_norm": 0.89453125,
      "learning_rate": 0.0002991822200392927,
      "loss": 1.2983,
      "step": 555
    },
    {
      "epoch": 0.2750153468385513,
      "grad_norm": 0.90234375,
      "learning_rate": 0.00029917485265225927,
      "loss": 1.2959,
      "step": 560
    },
    {
      "epoch": 0.2774708410067526,
      "grad_norm": 0.88671875,
      "learning_rate": 0.00029916748526522593,
      "loss": 1.2491,
      "step": 565
    },
    {
      "epoch": 0.27992633517495397,
      "grad_norm": 0.83984375,
      "learning_rate": 0.00029916011787819253,
      "loss": 1.2499,
      "step": 570
    },
    {
      "epoch": 0.2823818293431553,
      "grad_norm": 0.85546875,
      "learning_rate": 0.0002991527504911591,
      "loss": 1.2412,
      "step": 575
    },
    {
      "epoch": 0.28483732351135665,
      "grad_norm": 1.078125,
      "learning_rate": 0.00029914538310412574,
      "loss": 1.2628,
      "step": 580
    },
    {
      "epoch": 0.287292817679558,
      "grad_norm": 0.9375,
      "learning_rate": 0.0002991380157170923,
      "loss": 1.1997,
      "step": 585
    },
    {
      "epoch": 0.28974831184775934,
      "grad_norm": 1.09375,
      "learning_rate": 0.0002991306483300589,
      "loss": 1.232,
      "step": 590
    },
    {
      "epoch": 0.2922038060159607,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0002991232809430255,
      "loss": 1.364,
      "step": 595
    },
    {
      "epoch": 0.2946593001841621,
      "grad_norm": 0.953125,
      "learning_rate": 0.0002991159135559921,
      "loss": 1.2916,
      "step": 600
    },
    {
      "epoch": 0.29711479435236343,
      "grad_norm": 1.046875,
      "learning_rate": 0.0002991085461689587,
      "loss": 1.2197,
      "step": 605
    },
    {
      "epoch": 0.2995702885205648,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0002991011787819253,
      "loss": 1.2498,
      "step": 610
    },
    {
      "epoch": 0.3020257826887661,
      "grad_norm": 1.0,
      "learning_rate": 0.0002990938113948919,
      "loss": 1.1639,
      "step": 615
    },
    {
      "epoch": 0.30448127685696746,
      "grad_norm": 1.109375,
      "learning_rate": 0.00029908644400785853,
      "loss": 1.238,
      "step": 620
    },
    {
      "epoch": 0.3069367710251688,
      "grad_norm": 0.95703125,
      "learning_rate": 0.00029907907662082514,
      "loss": 1.248,
      "step": 625
    },
    {
      "epoch": 0.30939226519337015,
      "grad_norm": 1.109375,
      "learning_rate": 0.0002990717092337917,
      "loss": 1.2973,
      "step": 630
    },
    {
      "epoch": 0.3118477593615715,
      "grad_norm": 0.875,
      "learning_rate": 0.00029906434184675835,
      "loss": 1.2571,
      "step": 635
    },
    {
      "epoch": 0.31430325352977284,
      "grad_norm": 1.078125,
      "learning_rate": 0.00029905697445972495,
      "loss": 1.1488,
      "step": 640
    },
    {
      "epoch": 0.31675874769797424,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0002990496070726915,
      "loss": 1.2335,
      "step": 645
    },
    {
      "epoch": 0.3192142418661756,
      "grad_norm": 0.91796875,
      "learning_rate": 0.0002990422396856581,
      "loss": 1.2166,
      "step": 650
    },
    {
      "epoch": 0.32166973603437693,
      "grad_norm": 0.94140625,
      "learning_rate": 0.0002990348722986247,
      "loss": 1.2299,
      "step": 655
    },
    {
      "epoch": 0.3241252302025783,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0002990275049115913,
      "loss": 1.2503,
      "step": 660
    },
    {
      "epoch": 0.3265807243707796,
      "grad_norm": 1.0,
      "learning_rate": 0.0002990201375245579,
      "loss": 1.1958,
      "step": 665
    },
    {
      "epoch": 0.32903621853898096,
      "grad_norm": 1.0625,
      "learning_rate": 0.00029901277013752453,
      "loss": 1.234,
      "step": 670
    },
    {
      "epoch": 0.3314917127071823,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00029900540275049113,
      "loss": 1.2317,
      "step": 675
    },
    {
      "epoch": 0.33394720687538365,
      "grad_norm": 1.015625,
      "learning_rate": 0.00029899803536345774,
      "loss": 1.235,
      "step": 680
    },
    {
      "epoch": 0.336402701043585,
      "grad_norm": 1.015625,
      "learning_rate": 0.00029899066797642434,
      "loss": 1.1859,
      "step": 685
    },
    {
      "epoch": 0.3388581952117864,
      "grad_norm": 1.3125,
      "learning_rate": 0.00029898330058939095,
      "loss": 1.1574,
      "step": 690
    },
    {
      "epoch": 0.34131368937998774,
      "grad_norm": 1.09375,
      "learning_rate": 0.00029897593320235756,
      "loss": 1.1744,
      "step": 695
    },
    {
      "epoch": 0.3437691835481891,
      "grad_norm": 0.984375,
      "learning_rate": 0.00029896856581532416,
      "loss": 1.2072,
      "step": 700
    },
    {
      "epoch": 0.3462246777163904,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0002989611984282907,
      "loss": 1.2291,
      "step": 705
    },
    {
      "epoch": 0.34868017188459177,
      "grad_norm": 1.0625,
      "learning_rate": 0.00029895383104125737,
      "loss": 1.1692,
      "step": 710
    },
    {
      "epoch": 0.3511356660527931,
      "grad_norm": 1.0625,
      "learning_rate": 0.0002989464636542239,
      "loss": 1.16,
      "step": 715
    },
    {
      "epoch": 0.35359116022099446,
      "grad_norm": 1.0859375,
      "learning_rate": 0.00029893909626719053,
      "loss": 1.2039,
      "step": 720
    },
    {
      "epoch": 0.3560466543891958,
      "grad_norm": 1.0859375,
      "learning_rate": 0.00029893172888015713,
      "loss": 1.1863,
      "step": 725
    },
    {
      "epoch": 0.35850214855739715,
      "grad_norm": 0.98828125,
      "learning_rate": 0.00029892436149312374,
      "loss": 1.1905,
      "step": 730
    },
    {
      "epoch": 0.36095764272559855,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029891699410609034,
      "loss": 1.1314,
      "step": 735
    },
    {
      "epoch": 0.3634131368937999,
      "grad_norm": 1.046875,
      "learning_rate": 0.00029890962671905695,
      "loss": 1.2042,
      "step": 740
    },
    {
      "epoch": 0.36586863106200124,
      "grad_norm": 1.0703125,
      "learning_rate": 0.00029890225933202355,
      "loss": 1.1578,
      "step": 745
    },
    {
      "epoch": 0.3683241252302026,
      "grad_norm": 1.0703125,
      "learning_rate": 0.00029889489194499016,
      "loss": 1.1517,
      "step": 750
    },
    {
      "epoch": 0.3707796193984039,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029888752455795676,
      "loss": 1.1375,
      "step": 755
    },
    {
      "epoch": 0.37323511356660527,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0002988801571709233,
      "loss": 1.1986,
      "step": 760
    },
    {
      "epoch": 0.3756906077348066,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029887278978389,
      "loss": 1.1554,
      "step": 765
    },
    {
      "epoch": 0.37814610190300796,
      "grad_norm": 0.93359375,
      "learning_rate": 0.0002988654223968566,
      "loss": 1.2031,
      "step": 770
    },
    {
      "epoch": 0.38060159607120936,
      "grad_norm": 1.109375,
      "learning_rate": 0.00029885805500982313,
      "loss": 1.1379,
      "step": 775
    },
    {
      "epoch": 0.3830570902394107,
      "grad_norm": 1.0625,
      "learning_rate": 0.00029885068762278974,
      "loss": 1.1499,
      "step": 780
    },
    {
      "epoch": 0.38551258440761205,
      "grad_norm": 1.171875,
      "learning_rate": 0.00029884332023575634,
      "loss": 1.1419,
      "step": 785
    },
    {
      "epoch": 0.3879680785758134,
      "grad_norm": 1.046875,
      "learning_rate": 0.00029883595284872295,
      "loss": 1.1015,
      "step": 790
    },
    {
      "epoch": 0.39042357274401474,
      "grad_norm": 1.0703125,
      "learning_rate": 0.00029882858546168955,
      "loss": 1.1235,
      "step": 795
    },
    {
      "epoch": 0.3928790669122161,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029882121807465616,
      "loss": 1.1297,
      "step": 800
    },
    {
      "epoch": 0.3953345610804174,
      "grad_norm": 1.0625,
      "learning_rate": 0.00029881385068762276,
      "loss": 1.13,
      "step": 805
    },
    {
      "epoch": 0.39779005524861877,
      "grad_norm": 1.234375,
      "learning_rate": 0.00029880648330058937,
      "loss": 1.1297,
      "step": 810
    },
    {
      "epoch": 0.4002455494168201,
      "grad_norm": 1.125,
      "learning_rate": 0.00029879911591355597,
      "loss": 1.1219,
      "step": 815
    },
    {
      "epoch": 0.4027010435850215,
      "grad_norm": 1.046875,
      "learning_rate": 0.0002987917485265226,
      "loss": 1.1422,
      "step": 820
    },
    {
      "epoch": 0.40515653775322286,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0002987843811394892,
      "loss": 1.0821,
      "step": 825
    },
    {
      "epoch": 0.4076120319214242,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002987770137524558,
      "loss": 1.1259,
      "step": 830
    },
    {
      "epoch": 0.41006752608962554,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029876964636542234,
      "loss": 1.0789,
      "step": 835
    },
    {
      "epoch": 0.4125230202578269,
      "grad_norm": 1.0546875,
      "learning_rate": 0.000298762278978389,
      "loss": 1.1382,
      "step": 840
    },
    {
      "epoch": 0.41497851442602823,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00029875491159135555,
      "loss": 1.1515,
      "step": 845
    },
    {
      "epoch": 0.4174340085942296,
      "grad_norm": 1.0546875,
      "learning_rate": 0.00029874754420432215,
      "loss": 1.1768,
      "step": 850
    },
    {
      "epoch": 0.4198895027624309,
      "grad_norm": 1.15625,
      "learning_rate": 0.0002987401768172888,
      "loss": 1.1316,
      "step": 855
    },
    {
      "epoch": 0.42234499693063227,
      "grad_norm": 1.203125,
      "learning_rate": 0.00029873280943025536,
      "loss": 1.1454,
      "step": 860
    },
    {
      "epoch": 0.42480049109883367,
      "grad_norm": 1.0546875,
      "learning_rate": 0.00029872544204322197,
      "loss": 1.1371,
      "step": 865
    },
    {
      "epoch": 0.427255985267035,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0002987180746561886,
      "loss": 1.1536,
      "step": 870
    },
    {
      "epoch": 0.42971147943523635,
      "grad_norm": 1.21875,
      "learning_rate": 0.0002987107072691552,
      "loss": 1.1392,
      "step": 875
    },
    {
      "epoch": 0.4321669736034377,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0002987033398821218,
      "loss": 1.1926,
      "step": 880
    },
    {
      "epoch": 0.43462246777163904,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0002986959724950884,
      "loss": 1.0957,
      "step": 885
    },
    {
      "epoch": 0.4370779619398404,
      "grad_norm": 0.99609375,
      "learning_rate": 0.00029868860510805494,
      "loss": 1.0894,
      "step": 890
    },
    {
      "epoch": 0.43953345610804173,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0002986812377210216,
      "loss": 1.1189,
      "step": 895
    },
    {
      "epoch": 0.4419889502762431,
      "grad_norm": 1.109375,
      "learning_rate": 0.0002986738703339882,
      "loss": 1.0382,
      "step": 900
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.109375,
      "learning_rate": 0.00029866650294695476,
      "loss": 1.1667,
      "step": 905
    },
    {
      "epoch": 0.4468999386126458,
      "grad_norm": 1.1484375,
      "learning_rate": 0.0002986591355599214,
      "loss": 1.1153,
      "step": 910
    },
    {
      "epoch": 0.44935543278084716,
      "grad_norm": 1.125,
      "learning_rate": 0.000298651768172888,
      "loss": 1.07,
      "step": 915
    },
    {
      "epoch": 0.4518109269490485,
      "grad_norm": 1.0703125,
      "learning_rate": 0.00029864440078585457,
      "loss": 1.0966,
      "step": 920
    },
    {
      "epoch": 0.45426642111724985,
      "grad_norm": 1.1484375,
      "learning_rate": 0.0002986370333988212,
      "loss": 1.1078,
      "step": 925
    },
    {
      "epoch": 0.4567219152854512,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0002986296660117878,
      "loss": 1.0859,
      "step": 930
    },
    {
      "epoch": 0.45917740945365254,
      "grad_norm": 1.046875,
      "learning_rate": 0.0002986222986247544,
      "loss": 1.0093,
      "step": 935
    },
    {
      "epoch": 0.4616329036218539,
      "grad_norm": 1.0390625,
      "learning_rate": 0.000298614931237721,
      "loss": 1.0858,
      "step": 940
    },
    {
      "epoch": 0.46408839779005523,
      "grad_norm": 1.109375,
      "learning_rate": 0.0002986075638506876,
      "loss": 1.08,
      "step": 945
    },
    {
      "epoch": 0.4665438919582566,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0002986001964636542,
      "loss": 1.0661,
      "step": 950
    },
    {
      "epoch": 0.468999386126458,
      "grad_norm": 1.140625,
      "learning_rate": 0.0002985928290766208,
      "loss": 1.134,
      "step": 955
    },
    {
      "epoch": 0.4714548802946593,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0002985854616895874,
      "loss": 1.0319,
      "step": 960
    },
    {
      "epoch": 0.47391037446286066,
      "grad_norm": 1.1875,
      "learning_rate": 0.000298578094302554,
      "loss": 1.0839,
      "step": 965
    },
    {
      "epoch": 0.476365868631062,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002985707269155206,
      "loss": 1.0847,
      "step": 970
    },
    {
      "epoch": 0.47882136279926335,
      "grad_norm": 1.125,
      "learning_rate": 0.0002985633595284872,
      "loss": 1.1104,
      "step": 975
    },
    {
      "epoch": 0.4812768569674647,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0002985559921414538,
      "loss": 1.042,
      "step": 980
    },
    {
      "epoch": 0.48373235113566604,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00029854862475442044,
      "loss": 1.0451,
      "step": 985
    },
    {
      "epoch": 0.4861878453038674,
      "grad_norm": 1.1640625,
      "learning_rate": 0.000298541257367387,
      "loss": 1.1311,
      "step": 990
    },
    {
      "epoch": 0.4886433394720687,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0002985338899803536,
      "loss": 1.076,
      "step": 995
    },
    {
      "epoch": 0.4910988336402701,
      "grad_norm": 1.234375,
      "learning_rate": 0.0002985265225933202,
      "loss": 1.0827,
      "step": 1000
    },
    {
      "epoch": 0.49355432780847147,
      "grad_norm": 1.125,
      "learning_rate": 0.0002985191552062868,
      "loss": 1.0766,
      "step": 1005
    },
    {
      "epoch": 0.4960098219766728,
      "grad_norm": 1.3125,
      "learning_rate": 0.0002985117878192534,
      "loss": 1.0975,
      "step": 1010
    },
    {
      "epoch": 0.49846531614487416,
      "grad_norm": 1.1875,
      "learning_rate": 0.00029850442043222,
      "loss": 1.0893,
      "step": 1015
    },
    {
      "epoch": 0.5009208103130756,
      "grad_norm": 1.078125,
      "learning_rate": 0.0002984970530451866,
      "loss": 1.0654,
      "step": 1020
    },
    {
      "epoch": 0.5033763044812769,
      "grad_norm": 1.140625,
      "learning_rate": 0.00029848968565815323,
      "loss": 1.0219,
      "step": 1025
    },
    {
      "epoch": 0.5058317986494782,
      "grad_norm": 1.15625,
      "learning_rate": 0.00029848231827111983,
      "loss": 1.0596,
      "step": 1030
    },
    {
      "epoch": 0.5082872928176796,
      "grad_norm": 1.203125,
      "learning_rate": 0.0002984749508840864,
      "loss": 1.0474,
      "step": 1035
    },
    {
      "epoch": 0.5107427869858809,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029846758349705304,
      "loss": 1.0596,
      "step": 1040
    },
    {
      "epoch": 0.5131982811540823,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00029846021611001965,
      "loss": 1.0612,
      "step": 1045
    },
    {
      "epoch": 0.5156537753222836,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0002984528487229862,
      "loss": 1.0867,
      "step": 1050
    },
    {
      "epoch": 0.518109269490485,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0002984454813359528,
      "loss": 0.9771,
      "step": 1055
    },
    {
      "epoch": 0.5205647636586863,
      "grad_norm": 1.1875,
      "learning_rate": 0.0002984381139489194,
      "loss": 0.9625,
      "step": 1060
    },
    {
      "epoch": 0.5230202578268877,
      "grad_norm": 1.25,
      "learning_rate": 0.000298430746561886,
      "loss": 0.9896,
      "step": 1065
    },
    {
      "epoch": 0.525475751995089,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0002984233791748526,
      "loss": 1.0822,
      "step": 1070
    },
    {
      "epoch": 0.5279312461632903,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0002984160117878192,
      "loss": 1.004,
      "step": 1075
    },
    {
      "epoch": 0.5303867403314917,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00029840864440078583,
      "loss": 1.065,
      "step": 1080
    },
    {
      "epoch": 0.532842234499693,
      "grad_norm": 1.203125,
      "learning_rate": 0.00029840127701375244,
      "loss": 1.0459,
      "step": 1085
    },
    {
      "epoch": 0.5352977286678944,
      "grad_norm": 1.296875,
      "learning_rate": 0.00029839390962671904,
      "loss": 0.9934,
      "step": 1090
    },
    {
      "epoch": 0.5377532228360957,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00029838654223968565,
      "loss": 1.0348,
      "step": 1095
    },
    {
      "epoch": 0.5402087170042971,
      "grad_norm": 1.265625,
      "learning_rate": 0.00029837917485265225,
      "loss": 0.9787,
      "step": 1100
    },
    {
      "epoch": 0.5426642111724984,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0002983718074656188,
      "loss": 0.9445,
      "step": 1105
    },
    {
      "epoch": 0.5451197053406999,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0002983644400785854,
      "loss": 1.0281,
      "step": 1110
    },
    {
      "epoch": 0.5475751995089012,
      "grad_norm": 1.2890625,
      "learning_rate": 0.00029835707269155207,
      "loss": 0.9973,
      "step": 1115
    },
    {
      "epoch": 0.5500306936771026,
      "grad_norm": 1.234375,
      "learning_rate": 0.0002983497053045186,
      "loss": 1.0302,
      "step": 1120
    },
    {
      "epoch": 0.5524861878453039,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002983423379174852,
      "loss": 1.0053,
      "step": 1125
    },
    {
      "epoch": 0.5549416820135052,
      "grad_norm": 1.28125,
      "learning_rate": 0.00029833497053045183,
      "loss": 1.0323,
      "step": 1130
    },
    {
      "epoch": 0.5573971761817066,
      "grad_norm": 1.171875,
      "learning_rate": 0.00029832760314341843,
      "loss": 0.972,
      "step": 1135
    },
    {
      "epoch": 0.5598526703499079,
      "grad_norm": 1.25,
      "learning_rate": 0.00029832023575638504,
      "loss": 1.0228,
      "step": 1140
    },
    {
      "epoch": 0.5623081645181093,
      "grad_norm": 1.1875,
      "learning_rate": 0.00029831286836935164,
      "loss": 1.0079,
      "step": 1145
    },
    {
      "epoch": 0.5647636586863106,
      "grad_norm": 1.3125,
      "learning_rate": 0.00029830550098231825,
      "loss": 0.9817,
      "step": 1150
    },
    {
      "epoch": 0.567219152854512,
      "grad_norm": 1.21875,
      "learning_rate": 0.00029829813359528485,
      "loss": 1.0426,
      "step": 1155
    },
    {
      "epoch": 0.5696746470227133,
      "grad_norm": 1.3359375,
      "learning_rate": 0.00029829076620825146,
      "loss": 1.0559,
      "step": 1160
    },
    {
      "epoch": 0.5721301411909147,
      "grad_norm": 1.234375,
      "learning_rate": 0.000298283398821218,
      "loss": 0.9987,
      "step": 1165
    },
    {
      "epoch": 0.574585635359116,
      "grad_norm": 1.3046875,
      "learning_rate": 0.00029827603143418467,
      "loss": 0.9382,
      "step": 1170
    },
    {
      "epoch": 0.5770411295273173,
      "grad_norm": 1.03125,
      "learning_rate": 0.0002982686640471513,
      "loss": 0.9732,
      "step": 1175
    },
    {
      "epoch": 0.5794966236955187,
      "grad_norm": 1.375,
      "learning_rate": 0.0002982612966601178,
      "loss": 1.0156,
      "step": 1180
    },
    {
      "epoch": 0.58195211786372,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002982539292730845,
      "loss": 0.9294,
      "step": 1185
    },
    {
      "epoch": 0.5844076120319214,
      "grad_norm": 1.3125,
      "learning_rate": 0.00029824656188605104,
      "loss": 0.9621,
      "step": 1190
    },
    {
      "epoch": 0.5868631062001227,
      "grad_norm": 1.265625,
      "learning_rate": 0.00029823919449901764,
      "loss": 1.0176,
      "step": 1195
    },
    {
      "epoch": 0.5893186003683242,
      "grad_norm": 1.1640625,
      "learning_rate": 0.00029823182711198425,
      "loss": 0.957,
      "step": 1200
    },
    {
      "epoch": 0.5917740945365255,
      "grad_norm": 1.296875,
      "learning_rate": 0.00029822445972495085,
      "loss": 0.9746,
      "step": 1205
    },
    {
      "epoch": 0.5942295887047269,
      "grad_norm": 1.2890625,
      "learning_rate": 0.00029821709233791746,
      "loss": 1.0216,
      "step": 1210
    },
    {
      "epoch": 0.5966850828729282,
      "grad_norm": 1.2734375,
      "learning_rate": 0.00029820972495088406,
      "loss": 0.9004,
      "step": 1215
    },
    {
      "epoch": 0.5991405770411296,
      "grad_norm": 1.203125,
      "learning_rate": 0.00029820235756385067,
      "loss": 0.9797,
      "step": 1220
    },
    {
      "epoch": 0.6015960712093309,
      "grad_norm": 1.25,
      "learning_rate": 0.0002981949901768173,
      "loss": 0.9782,
      "step": 1225
    },
    {
      "epoch": 0.6040515653775322,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0002981876227897839,
      "loss": 0.9453,
      "step": 1230
    },
    {
      "epoch": 0.6065070595457336,
      "grad_norm": 1.2890625,
      "learning_rate": 0.00029818025540275043,
      "loss": 0.9956,
      "step": 1235
    },
    {
      "epoch": 0.6089625537139349,
      "grad_norm": 1.375,
      "learning_rate": 0.0002981728880157171,
      "loss": 0.9382,
      "step": 1240
    },
    {
      "epoch": 0.6114180478821363,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0002981655206286837,
      "loss": 0.9614,
      "step": 1245
    },
    {
      "epoch": 0.6138735420503376,
      "grad_norm": 1.3515625,
      "learning_rate": 0.00029815815324165025,
      "loss": 0.9435,
      "step": 1250
    },
    {
      "epoch": 0.616329036218539,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00029815078585461685,
      "loss": 0.9417,
      "step": 1255
    },
    {
      "epoch": 0.6187845303867403,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0002981434184675835,
      "loss": 0.9414,
      "step": 1260
    },
    {
      "epoch": 0.6212400245549416,
      "grad_norm": 1.2578125,
      "learning_rate": 0.00029813605108055006,
      "loss": 0.8788,
      "step": 1265
    },
    {
      "epoch": 0.623695518723143,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00029812868369351667,
      "loss": 0.9044,
      "step": 1270
    },
    {
      "epoch": 0.6261510128913443,
      "grad_norm": 1.328125,
      "learning_rate": 0.00029812131630648327,
      "loss": 0.9397,
      "step": 1275
    },
    {
      "epoch": 0.6286065070595457,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002981139489194499,
      "loss": 0.9649,
      "step": 1280
    },
    {
      "epoch": 0.6310620012277471,
      "grad_norm": 1.21875,
      "learning_rate": 0.0002981065815324165,
      "loss": 0.9586,
      "step": 1285
    },
    {
      "epoch": 0.6335174953959485,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0002980992141453831,
      "loss": 0.8832,
      "step": 1290
    },
    {
      "epoch": 0.6359729895641498,
      "grad_norm": 1.21875,
      "learning_rate": 0.0002980918467583497,
      "loss": 0.9181,
      "step": 1295
    },
    {
      "epoch": 0.6384284837323512,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002980844793713163,
      "loss": 0.9135,
      "step": 1300
    },
    {
      "epoch": 0.6408839779005525,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002980771119842829,
      "loss": 0.9208,
      "step": 1305
    },
    {
      "epoch": 0.6433394720687539,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00029806974459724945,
      "loss": 0.9387,
      "step": 1310
    },
    {
      "epoch": 0.6457949662369552,
      "grad_norm": 1.21875,
      "learning_rate": 0.0002980623772102161,
      "loss": 0.9658,
      "step": 1315
    },
    {
      "epoch": 0.6482504604051565,
      "grad_norm": 1.3125,
      "learning_rate": 0.00029805500982318266,
      "loss": 0.9377,
      "step": 1320
    },
    {
      "epoch": 0.6507059545733579,
      "grad_norm": 1.3203125,
      "learning_rate": 0.00029804764243614927,
      "loss": 0.9134,
      "step": 1325
    },
    {
      "epoch": 0.6531614487415592,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0002980402750491159,
      "loss": 0.8889,
      "step": 1330
    },
    {
      "epoch": 0.6556169429097606,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002980329076620825,
      "loss": 0.9462,
      "step": 1335
    },
    {
      "epoch": 0.6580724370779619,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0002980255402750491,
      "loss": 0.8807,
      "step": 1340
    },
    {
      "epoch": 0.6605279312461633,
      "grad_norm": 1.296875,
      "learning_rate": 0.0002980181728880157,
      "loss": 0.9537,
      "step": 1345
    },
    {
      "epoch": 0.6629834254143646,
      "grad_norm": 1.25,
      "learning_rate": 0.0002980108055009823,
      "loss": 0.9003,
      "step": 1350
    },
    {
      "epoch": 0.665438919582566,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0002980034381139489,
      "loss": 0.9597,
      "step": 1355
    },
    {
      "epoch": 0.6678944137507673,
      "grad_norm": 1.390625,
      "learning_rate": 0.0002979960707269155,
      "loss": 0.8969,
      "step": 1360
    },
    {
      "epoch": 0.6703499079189686,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0002979887033398821,
      "loss": 0.9744,
      "step": 1365
    },
    {
      "epoch": 0.67280540208717,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002979813359528487,
      "loss": 0.9199,
      "step": 1370
    },
    {
      "epoch": 0.6752608962553714,
      "grad_norm": 1.3125,
      "learning_rate": 0.0002979739685658153,
      "loss": 0.9096,
      "step": 1375
    },
    {
      "epoch": 0.6777163904235728,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00029796660117878187,
      "loss": 0.8887,
      "step": 1380
    },
    {
      "epoch": 0.6801718845917741,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002979592337917485,
      "loss": 0.9367,
      "step": 1385
    },
    {
      "epoch": 0.6826273787599755,
      "grad_norm": 1.2578125,
      "learning_rate": 0.00029795186640471514,
      "loss": 0.8993,
      "step": 1390
    },
    {
      "epoch": 0.6850828729281768,
      "grad_norm": 1.3671875,
      "learning_rate": 0.0002979444990176817,
      "loss": 0.8945,
      "step": 1395
    },
    {
      "epoch": 0.6875383670963782,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002979371316306483,
      "loss": 0.8375,
      "step": 1400
    },
    {
      "epoch": 0.6899938612645795,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0002979297642436149,
      "loss": 0.8946,
      "step": 1405
    },
    {
      "epoch": 0.6924493554327809,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002979223968565815,
      "loss": 0.9188,
      "step": 1410
    },
    {
      "epoch": 0.6949048496009822,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0002979150294695481,
      "loss": 0.9245,
      "step": 1415
    },
    {
      "epoch": 0.6973603437691835,
      "grad_norm": 1.25,
      "learning_rate": 0.0002979076620825147,
      "loss": 0.9176,
      "step": 1420
    },
    {
      "epoch": 0.6998158379373849,
      "grad_norm": 1.3046875,
      "learning_rate": 0.0002979002946954813,
      "loss": 0.931,
      "step": 1425
    },
    {
      "epoch": 0.7022713321055862,
      "grad_norm": 1.296875,
      "learning_rate": 0.0002978929273084479,
      "loss": 0.9221,
      "step": 1430
    },
    {
      "epoch": 0.7047268262737876,
      "grad_norm": 1.2578125,
      "learning_rate": 0.00029788555992141453,
      "loss": 0.9394,
      "step": 1435
    },
    {
      "epoch": 0.7071823204419889,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0002978781925343811,
      "loss": 0.9079,
      "step": 1440
    },
    {
      "epoch": 0.7096378146101903,
      "grad_norm": 1.265625,
      "learning_rate": 0.00029787082514734774,
      "loss": 0.8697,
      "step": 1445
    },
    {
      "epoch": 0.7120933087783916,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0002978634577603143,
      "loss": 0.8926,
      "step": 1450
    },
    {
      "epoch": 0.714548802946593,
      "grad_norm": 1.234375,
      "learning_rate": 0.0002978560903732809,
      "loss": 0.9028,
      "step": 1455
    },
    {
      "epoch": 0.7170042971147943,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029784872298624756,
      "loss": 0.8523,
      "step": 1460
    },
    {
      "epoch": 0.7194597912829958,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002978413555992141,
      "loss": 0.8939,
      "step": 1465
    },
    {
      "epoch": 0.7219152854511971,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002978339882121807,
      "loss": 0.9119,
      "step": 1470
    },
    {
      "epoch": 0.7243707796193984,
      "grad_norm": 1.21875,
      "learning_rate": 0.0002978266208251473,
      "loss": 0.9145,
      "step": 1475
    },
    {
      "epoch": 0.7268262737875998,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002978192534381139,
      "loss": 0.9027,
      "step": 1480
    },
    {
      "epoch": 0.7292817679558011,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00029781188605108053,
      "loss": 0.8813,
      "step": 1485
    },
    {
      "epoch": 0.7317372621240025,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029780451866404713,
      "loss": 0.9158,
      "step": 1490
    },
    {
      "epoch": 0.7341927562922038,
      "grad_norm": 1.359375,
      "learning_rate": 0.00029779715127701374,
      "loss": 0.8372,
      "step": 1495
    },
    {
      "epoch": 0.7366482504604052,
      "grad_norm": 1.234375,
      "learning_rate": 0.00029778978388998034,
      "loss": 0.8852,
      "step": 1500
    },
    {
      "epoch": 0.7391037446286065,
      "grad_norm": 1.2890625,
      "learning_rate": 0.00029778241650294695,
      "loss": 0.8594,
      "step": 1505
    },
    {
      "epoch": 0.7415592387968079,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002977750491159135,
      "loss": 0.8505,
      "step": 1510
    },
    {
      "epoch": 0.7440147329650092,
      "grad_norm": 1.265625,
      "learning_rate": 0.00029776768172888016,
      "loss": 0.8763,
      "step": 1515
    },
    {
      "epoch": 0.7464702271332105,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029776031434184676,
      "loss": 0.8427,
      "step": 1520
    },
    {
      "epoch": 0.7489257213014119,
      "grad_norm": 1.3671875,
      "learning_rate": 0.0002977529469548133,
      "loss": 0.9052,
      "step": 1525
    },
    {
      "epoch": 0.7513812154696132,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002977455795677799,
      "loss": 0.8731,
      "step": 1530
    },
    {
      "epoch": 0.7538367096378146,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002977382121807465,
      "loss": 0.8546,
      "step": 1535
    },
    {
      "epoch": 0.7562922038060159,
      "grad_norm": 1.265625,
      "learning_rate": 0.00029773084479371313,
      "loss": 0.8638,
      "step": 1540
    },
    {
      "epoch": 0.7587476979742173,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029772347740667974,
      "loss": 0.8777,
      "step": 1545
    },
    {
      "epoch": 0.7612031921424187,
      "grad_norm": 1.3203125,
      "learning_rate": 0.00029771611001964634,
      "loss": 0.8946,
      "step": 1550
    },
    {
      "epoch": 0.7636586863106201,
      "grad_norm": 1.28125,
      "learning_rate": 0.00029770874263261295,
      "loss": 0.888,
      "step": 1555
    },
    {
      "epoch": 0.7661141804788214,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029770137524557955,
      "loss": 0.885,
      "step": 1560
    },
    {
      "epoch": 0.7685696746470227,
      "grad_norm": 1.34375,
      "learning_rate": 0.00029769400785854616,
      "loss": 0.9073,
      "step": 1565
    },
    {
      "epoch": 0.7710251688152241,
      "grad_norm": 1.3125,
      "learning_rate": 0.00029768664047151276,
      "loss": 0.8535,
      "step": 1570
    },
    {
      "epoch": 0.7734806629834254,
      "grad_norm": 1.390625,
      "learning_rate": 0.00029767927308447937,
      "loss": 0.9081,
      "step": 1575
    },
    {
      "epoch": 0.7759361571516268,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0002976719056974459,
      "loss": 0.8488,
      "step": 1580
    },
    {
      "epoch": 0.7783916513198281,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002976645383104125,
      "loss": 0.8796,
      "step": 1585
    },
    {
      "epoch": 0.7808471454880295,
      "grad_norm": 1.296875,
      "learning_rate": 0.0002976571709233792,
      "loss": 0.8754,
      "step": 1590
    },
    {
      "epoch": 0.7833026396562308,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029764980353634573,
      "loss": 0.8608,
      "step": 1595
    },
    {
      "epoch": 0.7857581338244322,
      "grad_norm": 1.3125,
      "learning_rate": 0.00029764243614931234,
      "loss": 0.8516,
      "step": 1600
    },
    {
      "epoch": 0.7882136279926335,
      "grad_norm": 1.28125,
      "learning_rate": 0.00029763506876227894,
      "loss": 0.8612,
      "step": 1605
    },
    {
      "epoch": 0.7906691221608348,
      "grad_norm": 1.2734375,
      "learning_rate": 0.00029762770137524555,
      "loss": 0.878,
      "step": 1610
    },
    {
      "epoch": 0.7931246163290362,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00029762033398821215,
      "loss": 0.876,
      "step": 1615
    },
    {
      "epoch": 0.7955801104972375,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029761296660117876,
      "loss": 0.8602,
      "step": 1620
    },
    {
      "epoch": 0.7980356046654389,
      "grad_norm": 1.390625,
      "learning_rate": 0.00029760559921414536,
      "loss": 0.8899,
      "step": 1625
    },
    {
      "epoch": 0.8004910988336402,
      "grad_norm": 1.34375,
      "learning_rate": 0.00029759823182711197,
      "loss": 0.8607,
      "step": 1630
    },
    {
      "epoch": 0.8029465930018416,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0002975908644400786,
      "loss": 0.8635,
      "step": 1635
    },
    {
      "epoch": 0.805402087170043,
      "grad_norm": 1.3125,
      "learning_rate": 0.0002975834970530451,
      "loss": 0.9078,
      "step": 1640
    },
    {
      "epoch": 0.8078575813382444,
      "grad_norm": 1.25,
      "learning_rate": 0.0002975761296660118,
      "loss": 0.8193,
      "step": 1645
    },
    {
      "epoch": 0.8103130755064457,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002975687622789784,
      "loss": 0.8733,
      "step": 1650
    },
    {
      "epoch": 0.8127685696746471,
      "grad_norm": 1.328125,
      "learning_rate": 0.00029756139489194494,
      "loss": 0.8724,
      "step": 1655
    },
    {
      "epoch": 0.8152240638428484,
      "grad_norm": 1.2734375,
      "learning_rate": 0.00029755402750491155,
      "loss": 0.8293,
      "step": 1660
    },
    {
      "epoch": 0.8176795580110497,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029754666011787815,
      "loss": 0.8267,
      "step": 1665
    },
    {
      "epoch": 0.8201350521792511,
      "grad_norm": 1.296875,
      "learning_rate": 0.00029753929273084476,
      "loss": 0.7892,
      "step": 1670
    },
    {
      "epoch": 0.8225905463474524,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029753192534381136,
      "loss": 0.8449,
      "step": 1675
    },
    {
      "epoch": 0.8250460405156538,
      "grad_norm": 1.3203125,
      "learning_rate": 0.00029752455795677797,
      "loss": 0.8715,
      "step": 1680
    },
    {
      "epoch": 0.8275015346838551,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002975171905697446,
      "loss": 0.8735,
      "step": 1685
    },
    {
      "epoch": 0.8299570288520565,
      "grad_norm": 1.390625,
      "learning_rate": 0.0002975098231827112,
      "loss": 0.8116,
      "step": 1690
    },
    {
      "epoch": 0.8324125230202578,
      "grad_norm": 1.34375,
      "learning_rate": 0.0002975024557956778,
      "loss": 0.8771,
      "step": 1695
    },
    {
      "epoch": 0.8348680171884592,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002974950884086444,
      "loss": 0.8365,
      "step": 1700
    },
    {
      "epoch": 0.8373235113566605,
      "grad_norm": 1.2890625,
      "learning_rate": 0.000297487721021611,
      "loss": 0.8116,
      "step": 1705
    },
    {
      "epoch": 0.8397790055248618,
      "grad_norm": 1.265625,
      "learning_rate": 0.0002974803536345776,
      "loss": 0.8154,
      "step": 1710
    },
    {
      "epoch": 0.8422344996930632,
      "grad_norm": 1.375,
      "learning_rate": 0.00029747298624754415,
      "loss": 0.8485,
      "step": 1715
    },
    {
      "epoch": 0.8446899938612645,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002974656188605108,
      "loss": 0.8876,
      "step": 1720
    },
    {
      "epoch": 0.8471454880294659,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029745825147347736,
      "loss": 0.8355,
      "step": 1725
    },
    {
      "epoch": 0.8496009821976673,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029745088408644397,
      "loss": 0.8485,
      "step": 1730
    },
    {
      "epoch": 0.8520564763658687,
      "grad_norm": 1.265625,
      "learning_rate": 0.00029744351669941057,
      "loss": 0.8121,
      "step": 1735
    },
    {
      "epoch": 0.85451197053407,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002974361493123772,
      "loss": 0.8218,
      "step": 1740
    },
    {
      "epoch": 0.8569674647022714,
      "grad_norm": 1.28125,
      "learning_rate": 0.0002974287819253438,
      "loss": 0.7789,
      "step": 1745
    },
    {
      "epoch": 0.8594229588704727,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002974214145383104,
      "loss": 0.8006,
      "step": 1750
    },
    {
      "epoch": 0.861878453038674,
      "grad_norm": 1.5078125,
      "learning_rate": 0.000297414047151277,
      "loss": 0.8085,
      "step": 1755
    },
    {
      "epoch": 0.8643339472068754,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0002974066797642436,
      "loss": 0.8179,
      "step": 1760
    },
    {
      "epoch": 0.8667894413750767,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002973993123772102,
      "loss": 0.871,
      "step": 1765
    },
    {
      "epoch": 0.8692449355432781,
      "grad_norm": 1.3203125,
      "learning_rate": 0.00029739194499017675,
      "loss": 0.8199,
      "step": 1770
    },
    {
      "epoch": 0.8717004297114794,
      "grad_norm": 1.625,
      "learning_rate": 0.0002973845776031434,
      "loss": 0.8512,
      "step": 1775
    },
    {
      "epoch": 0.8741559238796808,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029737721021611,
      "loss": 0.799,
      "step": 1780
    },
    {
      "epoch": 0.8766114180478821,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029736984282907657,
      "loss": 0.8134,
      "step": 1785
    },
    {
      "epoch": 0.8790669122160835,
      "grad_norm": 1.3671875,
      "learning_rate": 0.0002973624754420432,
      "loss": 0.8528,
      "step": 1790
    },
    {
      "epoch": 0.8815224063842848,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002973551080550098,
      "loss": 0.7973,
      "step": 1795
    },
    {
      "epoch": 0.8839779005524862,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002973477406679764,
      "loss": 0.7962,
      "step": 1800
    },
    {
      "epoch": 0.8864333947206875,
      "grad_norm": 1.390625,
      "learning_rate": 0.000297340373280943,
      "loss": 0.7763,
      "step": 1805
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002973330058939096,
      "loss": 0.7674,
      "step": 1810
    },
    {
      "epoch": 0.8913443830570903,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002973256385068762,
      "loss": 0.7356,
      "step": 1815
    },
    {
      "epoch": 0.8937998772252916,
      "grad_norm": 1.375,
      "learning_rate": 0.0002973182711198428,
      "loss": 0.7899,
      "step": 1820
    },
    {
      "epoch": 0.896255371393493,
      "grad_norm": 1.375,
      "learning_rate": 0.0002973109037328094,
      "loss": 0.8067,
      "step": 1825
    },
    {
      "epoch": 0.8987108655616943,
      "grad_norm": 1.3671875,
      "learning_rate": 0.000297303536345776,
      "loss": 0.7861,
      "step": 1830
    },
    {
      "epoch": 0.9011663597298957,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002972961689587426,
      "loss": 0.7994,
      "step": 1835
    },
    {
      "epoch": 0.903621853898097,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002972888015717092,
      "loss": 0.8089,
      "step": 1840
    },
    {
      "epoch": 0.9060773480662984,
      "grad_norm": 1.375,
      "learning_rate": 0.0002972814341846758,
      "loss": 0.8105,
      "step": 1845
    },
    {
      "epoch": 0.9085328422344997,
      "grad_norm": 1.40625,
      "learning_rate": 0.00029727406679764244,
      "loss": 0.7558,
      "step": 1850
    },
    {
      "epoch": 0.910988336402701,
      "grad_norm": 1.4296875,
      "learning_rate": 0.000297266699410609,
      "loss": 0.7705,
      "step": 1855
    },
    {
      "epoch": 0.9134438305709024,
      "grad_norm": 1.5,
      "learning_rate": 0.0002972593320235756,
      "loss": 0.7933,
      "step": 1860
    },
    {
      "epoch": 0.9158993247391037,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029725196463654225,
      "loss": 0.8392,
      "step": 1865
    },
    {
      "epoch": 0.9183548189073051,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0002972445972495088,
      "loss": 0.7795,
      "step": 1870
    },
    {
      "epoch": 0.9208103130755064,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0002972372298624754,
      "loss": 0.7629,
      "step": 1875
    },
    {
      "epoch": 0.9232658072437078,
      "grad_norm": 1.296875,
      "learning_rate": 0.000297229862475442,
      "loss": 0.7438,
      "step": 1880
    },
    {
      "epoch": 0.9257213014119091,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002972224950884086,
      "loss": 0.8011,
      "step": 1885
    },
    {
      "epoch": 0.9281767955801105,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0002972151277013752,
      "loss": 0.8271,
      "step": 1890
    },
    {
      "epoch": 0.9306322897483118,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029720776031434183,
      "loss": 0.8095,
      "step": 1895
    },
    {
      "epoch": 0.9330877839165131,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002972003929273084,
      "loss": 0.7986,
      "step": 1900
    },
    {
      "epoch": 0.9355432780847146,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029719302554027504,
      "loss": 0.7887,
      "step": 1905
    },
    {
      "epoch": 0.937998772252916,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029718565815324164,
      "loss": 0.7719,
      "step": 1910
    },
    {
      "epoch": 0.9404542664211173,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002971782907662082,
      "loss": 0.8251,
      "step": 1915
    },
    {
      "epoch": 0.9429097605893186,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029717092337917486,
      "loss": 0.8236,
      "step": 1920
    },
    {
      "epoch": 0.94536525475752,
      "grad_norm": 1.34375,
      "learning_rate": 0.00029716355599214146,
      "loss": 0.7726,
      "step": 1925
    },
    {
      "epoch": 0.9478207489257213,
      "grad_norm": 1.3828125,
      "learning_rate": 0.000297156188605108,
      "loss": 0.7637,
      "step": 1930
    },
    {
      "epoch": 0.9502762430939227,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002971488212180746,
      "loss": 0.8105,
      "step": 1935
    },
    {
      "epoch": 0.952731737262124,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002971414538310412,
      "loss": 0.7671,
      "step": 1940
    },
    {
      "epoch": 0.9551872314303254,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002971340864440078,
      "loss": 0.7902,
      "step": 1945
    },
    {
      "epoch": 0.9576427255985267,
      "grad_norm": 1.34375,
      "learning_rate": 0.00029712671905697443,
      "loss": 0.762,
      "step": 1950
    },
    {
      "epoch": 0.960098219766728,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029711935166994104,
      "loss": 0.7519,
      "step": 1955
    },
    {
      "epoch": 0.9625537139349294,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029711198428290764,
      "loss": 0.7689,
      "step": 1960
    },
    {
      "epoch": 0.9650092081031307,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029710461689587425,
      "loss": 0.7977,
      "step": 1965
    },
    {
      "epoch": 0.9674647022713321,
      "grad_norm": 1.3359375,
      "learning_rate": 0.00029709724950884085,
      "loss": 0.7407,
      "step": 1970
    },
    {
      "epoch": 0.9699201964395334,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029708988212180746,
      "loss": 0.7918,
      "step": 1975
    },
    {
      "epoch": 0.9723756906077348,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029708251473477406,
      "loss": 0.7749,
      "step": 1980
    },
    {
      "epoch": 0.9748311847759361,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002970751473477406,
      "loss": 0.7909,
      "step": 1985
    },
    {
      "epoch": 0.9772866789441375,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002970677799607072,
      "loss": 0.7456,
      "step": 1990
    },
    {
      "epoch": 0.9797421731123389,
      "grad_norm": 1.34375,
      "learning_rate": 0.0002970604125736739,
      "loss": 0.7301,
      "step": 1995
    },
    {
      "epoch": 0.9821976672805403,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029705304518664043,
      "loss": 0.7862,
      "step": 2000
    },
    {
      "epoch": 0.9846531614487416,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029704567779960704,
      "loss": 0.734,
      "step": 2005
    },
    {
      "epoch": 0.9871086556169429,
      "grad_norm": 1.359375,
      "learning_rate": 0.00029703831041257364,
      "loss": 0.7813,
      "step": 2010
    },
    {
      "epoch": 0.9895641497851443,
      "grad_norm": 1.3515625,
      "learning_rate": 0.00029703094302554025,
      "loss": 0.827,
      "step": 2015
    },
    {
      "epoch": 0.9920196439533456,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029702357563850685,
      "loss": 0.7655,
      "step": 2020
    },
    {
      "epoch": 0.994475138121547,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029701620825147346,
      "loss": 0.7477,
      "step": 2025
    },
    {
      "epoch": 0.9969306322897483,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029700884086444006,
      "loss": 0.736,
      "step": 2030
    },
    {
      "epoch": 0.9993861264579497,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029700147347740667,
      "loss": 0.7712,
      "step": 2035
    },
    {
      "epoch": 1.0018416206261511,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029699410609037327,
      "loss": 0.6544,
      "step": 2040
    },
    {
      "epoch": 1.0042971147943525,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002969867387033398,
      "loss": 0.6378,
      "step": 2045
    },
    {
      "epoch": 1.0067526089625538,
      "grad_norm": 1.359375,
      "learning_rate": 0.0002969793713163065,
      "loss": 0.6543,
      "step": 2050
    },
    {
      "epoch": 1.0092081031307552,
      "grad_norm": 1.5,
      "learning_rate": 0.0002969720039292731,
      "loss": 0.6114,
      "step": 2055
    },
    {
      "epoch": 1.0116635972989565,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029696463654223964,
      "loss": 0.6528,
      "step": 2060
    },
    {
      "epoch": 1.0141190914671578,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029695726915520624,
      "loss": 0.641,
      "step": 2065
    },
    {
      "epoch": 1.0165745856353592,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029694990176817285,
      "loss": 0.639,
      "step": 2070
    },
    {
      "epoch": 1.0190300798035605,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029694253438113945,
      "loss": 0.6613,
      "step": 2075
    },
    {
      "epoch": 1.0214855739717619,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029693516699410606,
      "loss": 0.6329,
      "step": 2080
    },
    {
      "epoch": 1.0239410681399632,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029692779960707266,
      "loss": 0.6804,
      "step": 2085
    },
    {
      "epoch": 1.0263965623081646,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029692043222003927,
      "loss": 0.6334,
      "step": 2090
    },
    {
      "epoch": 1.028852056476366,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002969130648330059,
      "loss": 0.6314,
      "step": 2095
    },
    {
      "epoch": 1.0313075506445673,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002969056974459725,
      "loss": 0.6426,
      "step": 2100
    },
    {
      "epoch": 1.0337630448127686,
      "grad_norm": 1.40625,
      "learning_rate": 0.0002968983300589391,
      "loss": 0.6767,
      "step": 2105
    },
    {
      "epoch": 1.03621853898097,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002968909626719057,
      "loss": 0.6544,
      "step": 2110
    },
    {
      "epoch": 1.0386740331491713,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029688359528487224,
      "loss": 0.6536,
      "step": 2115
    },
    {
      "epoch": 1.0411295273173726,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029687622789783885,
      "loss": 0.6258,
      "step": 2120
    },
    {
      "epoch": 1.043585021485574,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002968688605108055,
      "loss": 0.6417,
      "step": 2125
    },
    {
      "epoch": 1.0460405156537753,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00029686149312377206,
      "loss": 0.6052,
      "step": 2130
    },
    {
      "epoch": 1.0484960098219767,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029685412573673866,
      "loss": 0.704,
      "step": 2135
    },
    {
      "epoch": 1.050951503990178,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029684675834970527,
      "loss": 0.64,
      "step": 2140
    },
    {
      "epoch": 1.0534069981583793,
      "grad_norm": 1.375,
      "learning_rate": 0.00029683939096267187,
      "loss": 0.6269,
      "step": 2145
    },
    {
      "epoch": 1.0558624923265807,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002968320235756385,
      "loss": 0.6634,
      "step": 2150
    },
    {
      "epoch": 1.058317986494782,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002968246561886051,
      "loss": 0.707,
      "step": 2155
    },
    {
      "epoch": 1.0607734806629834,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0002968172888015717,
      "loss": 0.6726,
      "step": 2160
    },
    {
      "epoch": 1.0632289748311847,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0002968099214145383,
      "loss": 0.6102,
      "step": 2165
    },
    {
      "epoch": 1.065684468999386,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002968025540275049,
      "loss": 0.6654,
      "step": 2170
    },
    {
      "epoch": 1.0681399631675874,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00029679518664047145,
      "loss": 0.6391,
      "step": 2175
    },
    {
      "epoch": 1.0705954573357888,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002967878192534381,
      "loss": 0.6263,
      "step": 2180
    },
    {
      "epoch": 1.07305095150399,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002967804518664047,
      "loss": 0.6611,
      "step": 2185
    },
    {
      "epoch": 1.0755064456721914,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029677308447937127,
      "loss": 0.665,
      "step": 2190
    },
    {
      "epoch": 1.0779619398403928,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002967657170923379,
      "loss": 0.6086,
      "step": 2195
    },
    {
      "epoch": 1.0804174340085941,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002967583497053045,
      "loss": 0.6824,
      "step": 2200
    },
    {
      "epoch": 1.0828729281767955,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002967509823182711,
      "loss": 0.6806,
      "step": 2205
    },
    {
      "epoch": 1.0853284223449968,
      "grad_norm": 1.390625,
      "learning_rate": 0.0002967436149312377,
      "loss": 0.6286,
      "step": 2210
    },
    {
      "epoch": 1.0877839165131982,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002967362475442043,
      "loss": 0.6203,
      "step": 2215
    },
    {
      "epoch": 1.0902394106813997,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002967288801571709,
      "loss": 0.6133,
      "step": 2220
    },
    {
      "epoch": 1.092694904849601,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002967215127701375,
      "loss": 0.623,
      "step": 2225
    },
    {
      "epoch": 1.0951503990178024,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002967141453831041,
      "loss": 0.6634,
      "step": 2230
    },
    {
      "epoch": 1.0976058931860038,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002967067779960707,
      "loss": 0.6337,
      "step": 2235
    },
    {
      "epoch": 1.1000613873542051,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002966994106090373,
      "loss": 0.6584,
      "step": 2240
    },
    {
      "epoch": 1.1025168815224065,
      "grad_norm": 1.40625,
      "learning_rate": 0.00029669204322200387,
      "loss": 0.6587,
      "step": 2245
    },
    {
      "epoch": 1.1049723756906078,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029668467583497053,
      "loss": 0.6782,
      "step": 2250
    },
    {
      "epoch": 1.1074278698588091,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029667730844793713,
      "loss": 0.6565,
      "step": 2255
    },
    {
      "epoch": 1.1098833640270105,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002966699410609037,
      "loss": 0.658,
      "step": 2260
    },
    {
      "epoch": 1.1123388581952118,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002966625736738703,
      "loss": 0.6581,
      "step": 2265
    },
    {
      "epoch": 1.1147943523634132,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029665520628683695,
      "loss": 0.6747,
      "step": 2270
    },
    {
      "epoch": 1.1172498465316145,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002966478388998035,
      "loss": 0.6374,
      "step": 2275
    },
    {
      "epoch": 1.1197053406998159,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002966404715127701,
      "loss": 0.625,
      "step": 2280
    },
    {
      "epoch": 1.1221608348680172,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002966331041257367,
      "loss": 0.6303,
      "step": 2285
    },
    {
      "epoch": 1.1246163290362186,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002966257367387033,
      "loss": 0.6222,
      "step": 2290
    },
    {
      "epoch": 1.12707182320442,
      "grad_norm": 1.390625,
      "learning_rate": 0.0002966183693516699,
      "loss": 0.6773,
      "step": 2295
    },
    {
      "epoch": 1.1295273173726212,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002966110019646365,
      "loss": 0.6349,
      "step": 2300
    },
    {
      "epoch": 1.1319828115408226,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029660363457760313,
      "loss": 0.6199,
      "step": 2305
    },
    {
      "epoch": 1.134438305709024,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029659626719056974,
      "loss": 0.622,
      "step": 2310
    },
    {
      "epoch": 1.1368937998772253,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029658889980353634,
      "loss": 0.609,
      "step": 2315
    },
    {
      "epoch": 1.1393492940454266,
      "grad_norm": 1.5,
      "learning_rate": 0.0002965815324165029,
      "loss": 0.6169,
      "step": 2320
    },
    {
      "epoch": 1.141804788213628,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029657416502946955,
      "loss": 0.6803,
      "step": 2325
    },
    {
      "epoch": 1.1442602823818293,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002965667976424361,
      "loss": 0.6769,
      "step": 2330
    },
    {
      "epoch": 1.1467157765500307,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0002965594302554027,
      "loss": 0.6555,
      "step": 2335
    },
    {
      "epoch": 1.149171270718232,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002965520628683693,
      "loss": 0.6901,
      "step": 2340
    },
    {
      "epoch": 1.1516267648864333,
      "grad_norm": 1.40625,
      "learning_rate": 0.0002965446954813359,
      "loss": 0.6711,
      "step": 2345
    },
    {
      "epoch": 1.1540822590546347,
      "grad_norm": 1.359375,
      "learning_rate": 0.0002965373280943025,
      "loss": 0.6087,
      "step": 2350
    },
    {
      "epoch": 1.156537753222836,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029652996070726913,
      "loss": 0.6314,
      "step": 2355
    },
    {
      "epoch": 1.1589932473910374,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00029652259332023573,
      "loss": 0.674,
      "step": 2360
    },
    {
      "epoch": 1.1614487415592387,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029651522593320234,
      "loss": 0.6301,
      "step": 2365
    },
    {
      "epoch": 1.16390423572744,
      "grad_norm": 1.375,
      "learning_rate": 0.00029650785854616894,
      "loss": 0.6401,
      "step": 2370
    },
    {
      "epoch": 1.1663597298956414,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029650049115913555,
      "loss": 0.635,
      "step": 2375
    },
    {
      "epoch": 1.168815224063843,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029649312377210215,
      "loss": 0.6345,
      "step": 2380
    },
    {
      "epoch": 1.1712707182320443,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029648575638506876,
      "loss": 0.6411,
      "step": 2385
    },
    {
      "epoch": 1.1737262124002457,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002964783889980353,
      "loss": 0.5983,
      "step": 2390
    },
    {
      "epoch": 1.176181706568447,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002964710216110019,
      "loss": 0.6773,
      "step": 2395
    },
    {
      "epoch": 1.1786372007366483,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002964636542239686,
      "loss": 0.6076,
      "step": 2400
    },
    {
      "epoch": 1.1810926949048497,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002964562868369351,
      "loss": 0.6564,
      "step": 2405
    },
    {
      "epoch": 1.183548189073051,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029644891944990173,
      "loss": 0.6233,
      "step": 2410
    },
    {
      "epoch": 1.1860036832412524,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029644155206286834,
      "loss": 0.6609,
      "step": 2415
    },
    {
      "epoch": 1.1884591774094537,
      "grad_norm": 1.390625,
      "learning_rate": 0.00029643418467583494,
      "loss": 0.6108,
      "step": 2420
    },
    {
      "epoch": 1.190914671577655,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00029642681728880155,
      "loss": 0.6271,
      "step": 2425
    },
    {
      "epoch": 1.1933701657458564,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029641944990176815,
      "loss": 0.6722,
      "step": 2430
    },
    {
      "epoch": 1.1958256599140578,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029641208251473476,
      "loss": 0.6657,
      "step": 2435
    },
    {
      "epoch": 1.198281154082259,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029640471512770136,
      "loss": 0.6162,
      "step": 2440
    },
    {
      "epoch": 1.2007366482504604,
      "grad_norm": 1.5,
      "learning_rate": 0.00029639734774066797,
      "loss": 0.6565,
      "step": 2445
    },
    {
      "epoch": 1.2031921424186618,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002963899803536345,
      "loss": 0.6939,
      "step": 2450
    },
    {
      "epoch": 1.2056476365868631,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0002963826129666012,
      "loss": 0.6108,
      "step": 2455
    },
    {
      "epoch": 1.2081031307550645,
      "grad_norm": 1.6875,
      "learning_rate": 0.00029637524557956773,
      "loss": 0.6198,
      "step": 2460
    },
    {
      "epoch": 1.2105586249232658,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029636787819253433,
      "loss": 0.6294,
      "step": 2465
    },
    {
      "epoch": 1.2130141190914672,
      "grad_norm": 1.4140625,
      "learning_rate": 0.000296360510805501,
      "loss": 0.6057,
      "step": 2470
    },
    {
      "epoch": 1.2154696132596685,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029635314341846755,
      "loss": 0.645,
      "step": 2475
    },
    {
      "epoch": 1.2179251074278699,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029634577603143415,
      "loss": 0.6695,
      "step": 2480
    },
    {
      "epoch": 1.2203806015960712,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029633840864440076,
      "loss": 0.6647,
      "step": 2485
    },
    {
      "epoch": 1.2228360957642725,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029633104125736736,
      "loss": 0.5974,
      "step": 2490
    },
    {
      "epoch": 1.225291589932474,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029632367387033397,
      "loss": 0.661,
      "step": 2495
    },
    {
      "epoch": 1.2277470841006752,
      "grad_norm": 1.375,
      "learning_rate": 0.00029631630648330057,
      "loss": 0.6657,
      "step": 2500
    },
    {
      "epoch": 1.2302025782688766,
      "grad_norm": 1.5,
      "learning_rate": 0.0002963089390962672,
      "loss": 0.6525,
      "step": 2505
    },
    {
      "epoch": 1.232658072437078,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002963015717092338,
      "loss": 0.6378,
      "step": 2510
    },
    {
      "epoch": 1.2351135666052793,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002962942043222004,
      "loss": 0.6454,
      "step": 2515
    },
    {
      "epoch": 1.2375690607734806,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029628683693516694,
      "loss": 0.6488,
      "step": 2520
    },
    {
      "epoch": 1.240024554941682,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002962794695481336,
      "loss": 0.6369,
      "step": 2525
    },
    {
      "epoch": 1.2424800491098833,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002962721021611002,
      "loss": 0.6387,
      "step": 2530
    },
    {
      "epoch": 1.2449355432780846,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029626473477406675,
      "loss": 0.5983,
      "step": 2535
    },
    {
      "epoch": 1.247391037446286,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029625736738703336,
      "loss": 0.6337,
      "step": 2540
    },
    {
      "epoch": 1.2498465316144873,
      "grad_norm": 1.296875,
      "learning_rate": 0.00029624999999999996,
      "loss": 0.6429,
      "step": 2545
    },
    {
      "epoch": 1.2523020257826887,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029624263261296657,
      "loss": 0.6298,
      "step": 2550
    },
    {
      "epoch": 1.25475751995089,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002962352652259332,
      "loss": 0.6507,
      "step": 2555
    },
    {
      "epoch": 1.2572130141190914,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002962278978388998,
      "loss": 0.6311,
      "step": 2560
    },
    {
      "epoch": 1.2596685082872927,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002962205304518664,
      "loss": 0.643,
      "step": 2565
    },
    {
      "epoch": 1.262124002455494,
      "grad_norm": 1.40625,
      "learning_rate": 0.000296213163064833,
      "loss": 0.6109,
      "step": 2570
    },
    {
      "epoch": 1.2645794966236954,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002962057956777996,
      "loss": 0.6112,
      "step": 2575
    },
    {
      "epoch": 1.2670349907918967,
      "grad_norm": 1.296875,
      "learning_rate": 0.0002961984282907662,
      "loss": 0.6109,
      "step": 2580
    },
    {
      "epoch": 1.269490484960098,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002961910609037328,
      "loss": 0.5709,
      "step": 2585
    },
    {
      "epoch": 1.2719459791282997,
      "grad_norm": 1.3359375,
      "learning_rate": 0.00029618369351669936,
      "loss": 0.5602,
      "step": 2590
    },
    {
      "epoch": 1.274401473296501,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029617632612966596,
      "loss": 0.6243,
      "step": 2595
    },
    {
      "epoch": 1.2768569674647023,
      "grad_norm": 1.640625,
      "learning_rate": 0.0002961689587426326,
      "loss": 0.5579,
      "step": 2600
    },
    {
      "epoch": 1.2793124616329037,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029616159135559917,
      "loss": 0.5962,
      "step": 2605
    },
    {
      "epoch": 1.281767955801105,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002961542239685658,
      "loss": 0.6843,
      "step": 2610
    },
    {
      "epoch": 1.2842234499693064,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002961468565815324,
      "loss": 0.6028,
      "step": 2615
    },
    {
      "epoch": 1.2866789441375077,
      "grad_norm": 1.4609375,
      "learning_rate": 0.000296139489194499,
      "loss": 0.6049,
      "step": 2620
    },
    {
      "epoch": 1.289134438305709,
      "grad_norm": 1.390625,
      "learning_rate": 0.0002961321218074656,
      "loss": 0.6755,
      "step": 2625
    },
    {
      "epoch": 1.2915899324739104,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002961247544204322,
      "loss": 0.5899,
      "step": 2630
    },
    {
      "epoch": 1.2940454266421118,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002961173870333988,
      "loss": 0.6025,
      "step": 2635
    },
    {
      "epoch": 1.296500920810313,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002961100196463654,
      "loss": 0.6334,
      "step": 2640
    },
    {
      "epoch": 1.2989564149785144,
      "grad_norm": 1.5,
      "learning_rate": 0.000296102652259332,
      "loss": 0.578,
      "step": 2645
    },
    {
      "epoch": 1.3014119091467158,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029609528487229856,
      "loss": 0.6569,
      "step": 2650
    },
    {
      "epoch": 1.3038674033149171,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002960879174852652,
      "loss": 0.6281,
      "step": 2655
    },
    {
      "epoch": 1.3063228974831185,
      "grad_norm": 1.359375,
      "learning_rate": 0.00029608055009823183,
      "loss": 0.6016,
      "step": 2660
    },
    {
      "epoch": 1.3087783916513198,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002960731827111984,
      "loss": 0.5842,
      "step": 2665
    },
    {
      "epoch": 1.3112338858195212,
      "grad_norm": 1.34375,
      "learning_rate": 0.000296065815324165,
      "loss": 0.6026,
      "step": 2670
    },
    {
      "epoch": 1.3136893799877225,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002960584479371316,
      "loss": 0.6227,
      "step": 2675
    },
    {
      "epoch": 1.3161448741559238,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002960510805500982,
      "loss": 0.6235,
      "step": 2680
    },
    {
      "epoch": 1.3186003683241252,
      "grad_norm": 1.375,
      "learning_rate": 0.0002960437131630648,
      "loss": 0.614,
      "step": 2685
    },
    {
      "epoch": 1.3210558624923265,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002960363457760314,
      "loss": 0.5854,
      "step": 2690
    },
    {
      "epoch": 1.3235113566605279,
      "grad_norm": 1.4921875,
      "learning_rate": 0.000296028978388998,
      "loss": 0.6484,
      "step": 2695
    },
    {
      "epoch": 1.3259668508287292,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002960216110019646,
      "loss": 0.6096,
      "step": 2700
    },
    {
      "epoch": 1.3284223449969306,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002960142436149312,
      "loss": 0.5863,
      "step": 2705
    },
    {
      "epoch": 1.330877839165132,
      "grad_norm": 1.5,
      "learning_rate": 0.00029600687622789783,
      "loss": 0.5996,
      "step": 2710
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.5,
      "learning_rate": 0.00029599950884086443,
      "loss": 0.6439,
      "step": 2715
    },
    {
      "epoch": 1.3357888275015346,
      "grad_norm": 1.40625,
      "learning_rate": 0.00029599214145383104,
      "loss": 0.6563,
      "step": 2720
    },
    {
      "epoch": 1.3382443216697362,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002959847740667976,
      "loss": 0.5779,
      "step": 2725
    },
    {
      "epoch": 1.3406998158379375,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029597740667976425,
      "loss": 0.5973,
      "step": 2730
    },
    {
      "epoch": 1.3431553100061389,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002959700392927308,
      "loss": 0.6127,
      "step": 2735
    },
    {
      "epoch": 1.3456108041743402,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002959626719056974,
      "loss": 0.605,
      "step": 2740
    },
    {
      "epoch": 1.3480662983425415,
      "grad_norm": 1.40625,
      "learning_rate": 0.000295955304518664,
      "loss": 0.589,
      "step": 2745
    },
    {
      "epoch": 1.350521792510743,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002959479371316306,
      "loss": 0.6092,
      "step": 2750
    },
    {
      "epoch": 1.3529772866789442,
      "grad_norm": 1.5,
      "learning_rate": 0.0002959405697445972,
      "loss": 0.6319,
      "step": 2755
    },
    {
      "epoch": 1.3554327808471456,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002959332023575638,
      "loss": 0.6166,
      "step": 2760
    },
    {
      "epoch": 1.357888275015347,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029592583497053043,
      "loss": 0.6302,
      "step": 2765
    },
    {
      "epoch": 1.3603437691835483,
      "grad_norm": 1.390625,
      "learning_rate": 0.00029591846758349704,
      "loss": 0.535,
      "step": 2770
    },
    {
      "epoch": 1.3627992633517496,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029591110019646364,
      "loss": 0.6236,
      "step": 2775
    },
    {
      "epoch": 1.365254757519951,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002959037328094302,
      "loss": 0.6273,
      "step": 2780
    },
    {
      "epoch": 1.3677102516881523,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029589636542239685,
      "loss": 0.6101,
      "step": 2785
    },
    {
      "epoch": 1.3701657458563536,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029588899803536346,
      "loss": 0.5911,
      "step": 2790
    },
    {
      "epoch": 1.372621240024555,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029588163064833,
      "loss": 0.5713,
      "step": 2795
    },
    {
      "epoch": 1.3750767341927563,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002958742632612966,
      "loss": 0.5994,
      "step": 2800
    },
    {
      "epoch": 1.3775322283609577,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002958668958742632,
      "loss": 0.5744,
      "step": 2805
    },
    {
      "epoch": 1.379987722529159,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002958595284872298,
      "loss": 0.5375,
      "step": 2810
    },
    {
      "epoch": 1.3824432166973604,
      "grad_norm": 1.625,
      "learning_rate": 0.00029585216110019643,
      "loss": 0.6286,
      "step": 2815
    },
    {
      "epoch": 1.3848987108655617,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029584479371316303,
      "loss": 0.6244,
      "step": 2820
    },
    {
      "epoch": 1.387354205033763,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029583742632612964,
      "loss": 0.5936,
      "step": 2825
    },
    {
      "epoch": 1.3898096992019644,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029583005893909624,
      "loss": 0.6118,
      "step": 2830
    },
    {
      "epoch": 1.3922651933701657,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029582269155206285,
      "loss": 0.6026,
      "step": 2835
    },
    {
      "epoch": 1.394720687538367,
      "grad_norm": 1.671875,
      "learning_rate": 0.00029581532416502945,
      "loss": 0.5624,
      "step": 2840
    },
    {
      "epoch": 1.3971761817065684,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029580795677799606,
      "loss": 0.5745,
      "step": 2845
    },
    {
      "epoch": 1.3996316758747698,
      "grad_norm": 1.625,
      "learning_rate": 0.00029580058939096266,
      "loss": 0.5617,
      "step": 2850
    },
    {
      "epoch": 1.4020871700429711,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002957932220039292,
      "loss": 0.5768,
      "step": 2855
    },
    {
      "epoch": 1.4045426642111725,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002957858546168959,
      "loss": 0.5938,
      "step": 2860
    },
    {
      "epoch": 1.4069981583793738,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002957784872298624,
      "loss": 0.5761,
      "step": 2865
    },
    {
      "epoch": 1.4094536525475752,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029577111984282903,
      "loss": 0.5666,
      "step": 2870
    },
    {
      "epoch": 1.4119091467157765,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002957637524557957,
      "loss": 0.5809,
      "step": 2875
    },
    {
      "epoch": 1.4143646408839778,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029575638506876224,
      "loss": 0.593,
      "step": 2880
    },
    {
      "epoch": 1.4168201350521792,
      "grad_norm": 1.6875,
      "learning_rate": 0.00029574901768172885,
      "loss": 0.6189,
      "step": 2885
    },
    {
      "epoch": 1.4192756292203805,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029574165029469545,
      "loss": 0.6563,
      "step": 2890
    },
    {
      "epoch": 1.4217311233885819,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029573428290766206,
      "loss": 0.5857,
      "step": 2895
    },
    {
      "epoch": 1.4241866175567832,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029572691552062866,
      "loss": 0.577,
      "step": 2900
    },
    {
      "epoch": 1.4266421117249846,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029571954813359527,
      "loss": 0.5501,
      "step": 2905
    },
    {
      "epoch": 1.429097605893186,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002957121807465618,
      "loss": 0.6033,
      "step": 2910
    },
    {
      "epoch": 1.4315531000613873,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002957048133595285,
      "loss": 0.619,
      "step": 2915
    },
    {
      "epoch": 1.4340085942295886,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002956974459724951,
      "loss": 0.6146,
      "step": 2920
    },
    {
      "epoch": 1.43646408839779,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029569007858546163,
      "loss": 0.6148,
      "step": 2925
    },
    {
      "epoch": 1.4389195825659913,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002956827111984283,
      "loss": 0.5519,
      "step": 2930
    },
    {
      "epoch": 1.4413750767341926,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029567534381139484,
      "loss": 0.5934,
      "step": 2935
    },
    {
      "epoch": 1.443830570902394,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029566797642436145,
      "loss": 0.524,
      "step": 2940
    },
    {
      "epoch": 1.4462860650705955,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029566060903732806,
      "loss": 0.612,
      "step": 2945
    },
    {
      "epoch": 1.4487415592387969,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029565324165029466,
      "loss": 0.5966,
      "step": 2950
    },
    {
      "epoch": 1.4511970534069982,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029564587426326127,
      "loss": 0.533,
      "step": 2955
    },
    {
      "epoch": 1.4536525475751996,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029563850687622787,
      "loss": 0.6076,
      "step": 2960
    },
    {
      "epoch": 1.456108041743401,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002956311394891945,
      "loss": 0.5744,
      "step": 2965
    },
    {
      "epoch": 1.4585635359116023,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002956237721021611,
      "loss": 0.585,
      "step": 2970
    },
    {
      "epoch": 1.4610190300798036,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0002956164047151277,
      "loss": 0.6109,
      "step": 2975
    },
    {
      "epoch": 1.463474524248005,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002956090373280943,
      "loss": 0.6246,
      "step": 2980
    },
    {
      "epoch": 1.4659300184162063,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002956016699410609,
      "loss": 0.6742,
      "step": 2985
    },
    {
      "epoch": 1.4683855125844076,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002955943025540275,
      "loss": 0.5898,
      "step": 2990
    },
    {
      "epoch": 1.470841006752609,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029558693516699405,
      "loss": 0.591,
      "step": 2995
    },
    {
      "epoch": 1.4732965009208103,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029557956777996066,
      "loss": 0.5889,
      "step": 3000
    },
    {
      "epoch": 1.4757519950890117,
      "grad_norm": 1.625,
      "learning_rate": 0.0002955722003929273,
      "loss": 0.6045,
      "step": 3005
    },
    {
      "epoch": 1.478207489257213,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029556483300589387,
      "loss": 0.5762,
      "step": 3010
    },
    {
      "epoch": 1.4806629834254144,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002955574656188605,
      "loss": 0.5969,
      "step": 3015
    },
    {
      "epoch": 1.4831184775936157,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002955500982318271,
      "loss": 0.568,
      "step": 3020
    },
    {
      "epoch": 1.485573971761817,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002955427308447937,
      "loss": 0.602,
      "step": 3025
    },
    {
      "epoch": 1.4880294659300184,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002955353634577603,
      "loss": 0.6184,
      "step": 3030
    },
    {
      "epoch": 1.4904849600982197,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002955279960707269,
      "loss": 0.6075,
      "step": 3035
    },
    {
      "epoch": 1.492940454266421,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002955206286836935,
      "loss": 0.6111,
      "step": 3040
    },
    {
      "epoch": 1.4953959484346224,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002955132612966601,
      "loss": 0.5551,
      "step": 3045
    },
    {
      "epoch": 1.4978514426028238,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002955058939096267,
      "loss": 0.5771,
      "step": 3050
    },
    {
      "epoch": 1.5003069367710253,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029549852652259326,
      "loss": 0.5685,
      "step": 3055
    },
    {
      "epoch": 1.5027624309392267,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002954911591355599,
      "loss": 0.5819,
      "step": 3060
    },
    {
      "epoch": 1.505217925107428,
      "grad_norm": 1.375,
      "learning_rate": 0.0002954837917485265,
      "loss": 0.5632,
      "step": 3065
    },
    {
      "epoch": 1.5076734192756294,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002954764243614931,
      "loss": 0.5909,
      "step": 3070
    },
    {
      "epoch": 1.5101289134438307,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002954690569744597,
      "loss": 0.5464,
      "step": 3075
    },
    {
      "epoch": 1.512584407612032,
      "grad_norm": 1.625,
      "learning_rate": 0.0002954616895874263,
      "loss": 0.5877,
      "step": 3080
    },
    {
      "epoch": 1.5150399017802334,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002954543222003929,
      "loss": 0.5476,
      "step": 3085
    },
    {
      "epoch": 1.5174953959484347,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002954469548133595,
      "loss": 0.6249,
      "step": 3090
    },
    {
      "epoch": 1.519950890116636,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002954395874263261,
      "loss": 0.6161,
      "step": 3095
    },
    {
      "epoch": 1.5224063842848374,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002954322200392927,
      "loss": 0.5439,
      "step": 3100
    },
    {
      "epoch": 1.5248618784530388,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002954248526522593,
      "loss": 0.57,
      "step": 3105
    },
    {
      "epoch": 1.5273173726212401,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002954174852652259,
      "loss": 0.5285,
      "step": 3110
    },
    {
      "epoch": 1.5297728667894415,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002954101178781925,
      "loss": 0.5611,
      "step": 3115
    },
    {
      "epoch": 1.5322283609576428,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029540275049115913,
      "loss": 0.5372,
      "step": 3120
    },
    {
      "epoch": 1.5346838551258442,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002953953831041257,
      "loss": 0.5586,
      "step": 3125
    },
    {
      "epoch": 1.5371393492940455,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002953880157170923,
      "loss": 0.5491,
      "step": 3130
    },
    {
      "epoch": 1.5395948434622468,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029538064833005894,
      "loss": 0.5625,
      "step": 3135
    },
    {
      "epoch": 1.5420503376304482,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002953732809430255,
      "loss": 0.543,
      "step": 3140
    },
    {
      "epoch": 1.5445058317986495,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002953659135559921,
      "loss": 0.5908,
      "step": 3145
    },
    {
      "epoch": 1.5469613259668509,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002953585461689587,
      "loss": 0.5926,
      "step": 3150
    },
    {
      "epoch": 1.5494168201350522,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002953511787819253,
      "loss": 0.5666,
      "step": 3155
    },
    {
      "epoch": 1.5518723143032536,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002953438113948919,
      "loss": 0.584,
      "step": 3160
    },
    {
      "epoch": 1.554327808471455,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002953364440078585,
      "loss": 0.5516,
      "step": 3165
    },
    {
      "epoch": 1.5567833026396563,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002953290766208251,
      "loss": 0.5373,
      "step": 3170
    },
    {
      "epoch": 1.5592387968078576,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029532170923379173,
      "loss": 0.5461,
      "step": 3175
    },
    {
      "epoch": 1.561694290976059,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029531434184675834,
      "loss": 0.5789,
      "step": 3180
    },
    {
      "epoch": 1.5641497851442603,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002953069744597249,
      "loss": 0.6077,
      "step": 3185
    },
    {
      "epoch": 1.5666052793124616,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029529960707269155,
      "loss": 0.5762,
      "step": 3190
    },
    {
      "epoch": 1.569060773480663,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029529223968565815,
      "loss": 0.586,
      "step": 3195
    },
    {
      "epoch": 1.5715162676488643,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002952848722986247,
      "loss": 0.5607,
      "step": 3200
    },
    {
      "epoch": 1.5739717618170657,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029527750491159136,
      "loss": 0.5895,
      "step": 3205
    },
    {
      "epoch": 1.576427255985267,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002952701375245579,
      "loss": 0.5503,
      "step": 3210
    },
    {
      "epoch": 1.5788827501534684,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002952627701375245,
      "loss": 0.5482,
      "step": 3215
    },
    {
      "epoch": 1.5813382443216697,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002952554027504911,
      "loss": 0.5325,
      "step": 3220
    },
    {
      "epoch": 1.583793738489871,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029524803536345773,
      "loss": 0.545,
      "step": 3225
    },
    {
      "epoch": 1.5862492326580724,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029524066797642434,
      "loss": 0.5623,
      "step": 3230
    },
    {
      "epoch": 1.5887047268262737,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029523330058939094,
      "loss": 0.5722,
      "step": 3235
    },
    {
      "epoch": 1.591160220994475,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029522593320235755,
      "loss": 0.5316,
      "step": 3240
    },
    {
      "epoch": 1.5936157151626764,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029521856581532415,
      "loss": 0.539,
      "step": 3245
    },
    {
      "epoch": 1.5960712093308778,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029521119842829076,
      "loss": 0.573,
      "step": 3250
    },
    {
      "epoch": 1.598526703499079,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002952038310412573,
      "loss": 0.5291,
      "step": 3255
    },
    {
      "epoch": 1.6009821976672804,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029519646365422397,
      "loss": 0.5784,
      "step": 3260
    },
    {
      "epoch": 1.6034376918354818,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029518909626719057,
      "loss": 0.5608,
      "step": 3265
    },
    {
      "epoch": 1.6058931860036831,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002951817288801571,
      "loss": 0.5559,
      "step": 3270
    },
    {
      "epoch": 1.6083486801718845,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029517436149312373,
      "loss": 0.5589,
      "step": 3275
    },
    {
      "epoch": 1.6108041743400858,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029516699410609033,
      "loss": 0.5972,
      "step": 3280
    },
    {
      "epoch": 1.6132596685082872,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029515962671905694,
      "loss": 0.5941,
      "step": 3285
    },
    {
      "epoch": 1.6157151626764885,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029515225933202354,
      "loss": 0.5541,
      "step": 3290
    },
    {
      "epoch": 1.6181706568446899,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029514489194499015,
      "loss": 0.5727,
      "step": 3295
    },
    {
      "epoch": 1.6206261510128912,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029513752455795675,
      "loss": 0.5433,
      "step": 3300
    },
    {
      "epoch": 1.6230816451810925,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029513015717092336,
      "loss": 0.545,
      "step": 3305
    },
    {
      "epoch": 1.625537139349294,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029512278978388996,
      "loss": 0.5727,
      "step": 3310
    },
    {
      "epoch": 1.6279926335174952,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029511542239685657,
      "loss": 0.548,
      "step": 3315
    },
    {
      "epoch": 1.6304481276856968,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002951080550098232,
      "loss": 0.5659,
      "step": 3320
    },
    {
      "epoch": 1.6329036218538981,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002951006876227898,
      "loss": 0.5753,
      "step": 3325
    },
    {
      "epoch": 1.6353591160220995,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00029509332023575633,
      "loss": 0.5666,
      "step": 3330
    },
    {
      "epoch": 1.6378146101903008,
      "grad_norm": 1.4609375,
      "learning_rate": 0.000295085952848723,
      "loss": 0.5829,
      "step": 3335
    },
    {
      "epoch": 1.6402701043585022,
      "grad_norm": 1.34375,
      "learning_rate": 0.00029507858546168954,
      "loss": 0.5504,
      "step": 3340
    },
    {
      "epoch": 1.6427255985267035,
      "grad_norm": 1.6875,
      "learning_rate": 0.00029507121807465615,
      "loss": 0.5757,
      "step": 3345
    },
    {
      "epoch": 1.6451810926949049,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029506385068762275,
      "loss": 0.5634,
      "step": 3350
    },
    {
      "epoch": 1.6476365868631062,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029505648330058936,
      "loss": 0.6062,
      "step": 3355
    },
    {
      "epoch": 1.6500920810313076,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029504911591355596,
      "loss": 0.517,
      "step": 3360
    },
    {
      "epoch": 1.652547575199509,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029504174852652257,
      "loss": 0.5599,
      "step": 3365
    },
    {
      "epoch": 1.6550030693677102,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029503438113948917,
      "loss": 0.5897,
      "step": 3370
    },
    {
      "epoch": 1.6574585635359116,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002950270137524558,
      "loss": 0.5362,
      "step": 3375
    },
    {
      "epoch": 1.659914057704113,
      "grad_norm": 1.5,
      "learning_rate": 0.0002950196463654224,
      "loss": 0.5428,
      "step": 3380
    },
    {
      "epoch": 1.6623695518723143,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029501227897838893,
      "loss": 0.5312,
      "step": 3385
    },
    {
      "epoch": 1.6648250460405156,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002950049115913556,
      "loss": 0.5159,
      "step": 3390
    },
    {
      "epoch": 1.667280540208717,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002949975442043222,
      "loss": 0.5731,
      "step": 3395
    },
    {
      "epoch": 1.6697360343769183,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029499017681728875,
      "loss": 0.5393,
      "step": 3400
    },
    {
      "epoch": 1.6721915285451197,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029498280943025535,
      "loss": 0.622,
      "step": 3405
    },
    {
      "epoch": 1.6746470227133212,
      "grad_norm": 1.3984375,
      "learning_rate": 0.000294975442043222,
      "loss": 0.5499,
      "step": 3410
    },
    {
      "epoch": 1.6771025168815226,
      "grad_norm": 1.625,
      "learning_rate": 0.00029496807465618857,
      "loss": 0.509,
      "step": 3415
    },
    {
      "epoch": 1.679558011049724,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029496070726915517,
      "loss": 0.5407,
      "step": 3420
    },
    {
      "epoch": 1.6820135052179253,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002949533398821218,
      "loss": 0.5194,
      "step": 3425
    },
    {
      "epoch": 1.6844689993861266,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002949459724950884,
      "loss": 0.5745,
      "step": 3430
    },
    {
      "epoch": 1.686924493554328,
      "grad_norm": 1.5234375,
      "learning_rate": 0.000294938605108055,
      "loss": 0.5716,
      "step": 3435
    },
    {
      "epoch": 1.6893799877225293,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002949312377210216,
      "loss": 0.5491,
      "step": 3440
    },
    {
      "epoch": 1.6918354818907306,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002949238703339882,
      "loss": 0.5702,
      "step": 3445
    },
    {
      "epoch": 1.694290976058932,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002949165029469548,
      "loss": 0.5389,
      "step": 3450
    },
    {
      "epoch": 1.6967464702271333,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002949091355599214,
      "loss": 0.5429,
      "step": 3455
    },
    {
      "epoch": 1.6992019643953347,
      "grad_norm": 1.6328125,
      "learning_rate": 0.00029490176817288796,
      "loss": 0.5341,
      "step": 3460
    },
    {
      "epoch": 1.701657458563536,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002948944007858546,
      "loss": 0.577,
      "step": 3465
    },
    {
      "epoch": 1.7041129527317374,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029488703339882117,
      "loss": 0.6142,
      "step": 3470
    },
    {
      "epoch": 1.7065684468999387,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002948796660117878,
      "loss": 0.5394,
      "step": 3475
    },
    {
      "epoch": 1.70902394106814,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029487229862475443,
      "loss": 0.5919,
      "step": 3480
    },
    {
      "epoch": 1.7114794352363414,
      "grad_norm": 1.6171875,
      "learning_rate": 0.000294864931237721,
      "loss": 0.5544,
      "step": 3485
    },
    {
      "epoch": 1.7139349294045427,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002948575638506876,
      "loss": 0.5325,
      "step": 3490
    },
    {
      "epoch": 1.716390423572744,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002948501964636542,
      "loss": 0.5767,
      "step": 3495
    },
    {
      "epoch": 1.7188459177409454,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002948428290766208,
      "loss": 0.5742,
      "step": 3500
    },
    {
      "epoch": 1.7213014119091468,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002948354616895874,
      "loss": 0.5612,
      "step": 3505
    },
    {
      "epoch": 1.723756906077348,
      "grad_norm": 1.546875,
      "learning_rate": 0.000294828094302554,
      "loss": 0.5674,
      "step": 3510
    },
    {
      "epoch": 1.7262124002455494,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002948207269155206,
      "loss": 0.5768,
      "step": 3515
    },
    {
      "epoch": 1.7286678944137508,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002948133595284872,
      "loss": 0.5032,
      "step": 3520
    },
    {
      "epoch": 1.7311233885819521,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002948059921414538,
      "loss": 0.5514,
      "step": 3525
    },
    {
      "epoch": 1.7335788827501535,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002947986247544204,
      "loss": 0.5514,
      "step": 3530
    },
    {
      "epoch": 1.7360343769183548,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029479125736738704,
      "loss": 0.5423,
      "step": 3535
    },
    {
      "epoch": 1.7384898710865562,
      "grad_norm": 1.65625,
      "learning_rate": 0.00029478388998035364,
      "loss": 0.5212,
      "step": 3540
    },
    {
      "epoch": 1.7409453652547575,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002947765225933202,
      "loss": 0.5132,
      "step": 3545
    },
    {
      "epoch": 1.7434008594229589,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0002947691552062868,
      "loss": 0.5512,
      "step": 3550
    },
    {
      "epoch": 1.7458563535911602,
      "grad_norm": 1.625,
      "learning_rate": 0.0002947617878192534,
      "loss": 0.5566,
      "step": 3555
    },
    {
      "epoch": 1.7483118477593615,
      "grad_norm": 1.71875,
      "learning_rate": 0.00029475442043222,
      "loss": 0.5685,
      "step": 3560
    },
    {
      "epoch": 1.750767341927563,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002947470530451866,
      "loss": 0.5301,
      "step": 3565
    },
    {
      "epoch": 1.7532228360957642,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002947396856581532,
      "loss": 0.5512,
      "step": 3570
    },
    {
      "epoch": 1.7556783302639656,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002947323182711198,
      "loss": 0.5417,
      "step": 3575
    },
    {
      "epoch": 1.758133824432167,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029472495088408643,
      "loss": 0.5235,
      "step": 3580
    },
    {
      "epoch": 1.7605893186003683,
      "grad_norm": 1.5,
      "learning_rate": 0.00029471758349705303,
      "loss": 0.5493,
      "step": 3585
    },
    {
      "epoch": 1.7630448127685696,
      "grad_norm": 1.7109375,
      "learning_rate": 0.00029471021611001964,
      "loss": 0.5543,
      "step": 3590
    },
    {
      "epoch": 1.765500306936771,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029470284872298624,
      "loss": 0.5532,
      "step": 3595
    },
    {
      "epoch": 1.7679558011049723,
      "grad_norm": 1.40625,
      "learning_rate": 0.0002946954813359528,
      "loss": 0.5095,
      "step": 3600
    },
    {
      "epoch": 1.7704112952731736,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002946881139489194,
      "loss": 0.5494,
      "step": 3605
    },
    {
      "epoch": 1.772866789441375,
      "grad_norm": 1.5,
      "learning_rate": 0.00029468074656188606,
      "loss": 0.5522,
      "step": 3610
    },
    {
      "epoch": 1.7753222836095763,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002946733791748526,
      "loss": 0.5631,
      "step": 3615
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002946660117878192,
      "loss": 0.5305,
      "step": 3620
    },
    {
      "epoch": 1.780233271945979,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002946586444007858,
      "loss": 0.5641,
      "step": 3625
    },
    {
      "epoch": 1.7826887661141804,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002946512770137524,
      "loss": 0.5432,
      "step": 3630
    },
    {
      "epoch": 1.7851442602823817,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029464390962671903,
      "loss": 0.5648,
      "step": 3635
    },
    {
      "epoch": 1.787599754450583,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029463654223968564,
      "loss": 0.5275,
      "step": 3640
    },
    {
      "epoch": 1.7900552486187844,
      "grad_norm": 1.734375,
      "learning_rate": 0.00029462917485265224,
      "loss": 0.5414,
      "step": 3645
    },
    {
      "epoch": 1.7925107427869857,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029462180746561885,
      "loss": 0.5159,
      "step": 3650
    },
    {
      "epoch": 1.794966236955187,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029461444007858545,
      "loss": 0.5017,
      "step": 3655
    },
    {
      "epoch": 1.7974217311233884,
      "grad_norm": 1.421875,
      "learning_rate": 0.000294607072691552,
      "loss": 0.5285,
      "step": 3660
    },
    {
      "epoch": 1.7998772252915898,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029459970530451866,
      "loss": 0.5508,
      "step": 3665
    },
    {
      "epoch": 1.8023327194597911,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029459233791748527,
      "loss": 0.5793,
      "step": 3670
    },
    {
      "epoch": 1.8047882136279927,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002945849705304518,
      "loss": 0.5498,
      "step": 3675
    },
    {
      "epoch": 1.807243707796194,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002945776031434184,
      "loss": 0.5426,
      "step": 3680
    },
    {
      "epoch": 1.8096992019643954,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029457023575638503,
      "loss": 0.5284,
      "step": 3685
    },
    {
      "epoch": 1.8121546961325967,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029456286836935163,
      "loss": 0.5528,
      "step": 3690
    },
    {
      "epoch": 1.814610190300798,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029455550098231824,
      "loss": 0.531,
      "step": 3695
    },
    {
      "epoch": 1.8170656844689994,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029454813359528485,
      "loss": 0.5475,
      "step": 3700
    },
    {
      "epoch": 1.8195211786372008,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029454076620825145,
      "loss": 0.5387,
      "step": 3705
    },
    {
      "epoch": 1.821976672805402,
      "grad_norm": 1.625,
      "learning_rate": 0.00029453339882121806,
      "loss": 0.5414,
      "step": 3710
    },
    {
      "epoch": 1.8244321669736034,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029452603143418466,
      "loss": 0.5713,
      "step": 3715
    },
    {
      "epoch": 1.8268876611418048,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029451866404715127,
      "loss": 0.5299,
      "step": 3720
    },
    {
      "epoch": 1.8293431553100061,
      "grad_norm": 1.6875,
      "learning_rate": 0.00029451129666011787,
      "loss": 0.5586,
      "step": 3725
    },
    {
      "epoch": 1.8317986494782075,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002945039292730844,
      "loss": 0.5254,
      "step": 3730
    },
    {
      "epoch": 1.8342541436464088,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029449656188605103,
      "loss": 0.5329,
      "step": 3735
    },
    {
      "epoch": 1.8367096378146102,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002944891944990177,
      "loss": 0.5034,
      "step": 3740
    },
    {
      "epoch": 1.8391651319828115,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029448182711198424,
      "loss": 0.5501,
      "step": 3745
    },
    {
      "epoch": 1.8416206261510129,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00029447445972495084,
      "loss": 0.5596,
      "step": 3750
    },
    {
      "epoch": 1.8440761203192142,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029446709233791745,
      "loss": 0.5363,
      "step": 3755
    },
    {
      "epoch": 1.8465316144874155,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029445972495088405,
      "loss": 0.5446,
      "step": 3760
    },
    {
      "epoch": 1.848987108655617,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029445235756385066,
      "loss": 0.553,
      "step": 3765
    },
    {
      "epoch": 1.8514426028238185,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029444499017681726,
      "loss": 0.5249,
      "step": 3770
    },
    {
      "epoch": 1.8538980969920198,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029443762278978387,
      "loss": 0.5081,
      "step": 3775
    },
    {
      "epoch": 1.8563535911602211,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002944302554027505,
      "loss": 0.5689,
      "step": 3780
    },
    {
      "epoch": 1.8588090853284225,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002944228880157171,
      "loss": 0.5363,
      "step": 3785
    },
    {
      "epoch": 1.8612645794966238,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029441552062868363,
      "loss": 0.535,
      "step": 3790
    },
    {
      "epoch": 1.8637200736648252,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002944081532416503,
      "loss": 0.5404,
      "step": 3795
    },
    {
      "epoch": 1.8661755678330265,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002944007858546169,
      "loss": 0.5505,
      "step": 3800
    },
    {
      "epoch": 1.8686310620012279,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029439341846758345,
      "loss": 0.5123,
      "step": 3805
    },
    {
      "epoch": 1.8710865561694292,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029438605108055005,
      "loss": 0.4964,
      "step": 3810
    },
    {
      "epoch": 1.8735420503376305,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029437868369351666,
      "loss": 0.5153,
      "step": 3815
    },
    {
      "epoch": 1.875997544505832,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029437131630648326,
      "loss": 0.5325,
      "step": 3820
    },
    {
      "epoch": 1.8784530386740332,
      "grad_norm": 1.5,
      "learning_rate": 0.00029436394891944987,
      "loss": 0.5373,
      "step": 3825
    },
    {
      "epoch": 1.8809085328422346,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029435658153241647,
      "loss": 0.521,
      "step": 3830
    },
    {
      "epoch": 1.883364027010436,
      "grad_norm": 1.828125,
      "learning_rate": 0.0002943492141453831,
      "loss": 0.509,
      "step": 3835
    },
    {
      "epoch": 1.8858195211786373,
      "grad_norm": 1.625,
      "learning_rate": 0.0002943418467583497,
      "loss": 0.5112,
      "step": 3840
    },
    {
      "epoch": 1.8882750153468386,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002943344793713163,
      "loss": 0.4975,
      "step": 3845
    },
    {
      "epoch": 1.89073050951504,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002943271119842829,
      "loss": 0.5127,
      "step": 3850
    },
    {
      "epoch": 1.8931860036832413,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002943197445972495,
      "loss": 0.5188,
      "step": 3855
    },
    {
      "epoch": 1.8956414978514426,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002943123772102161,
      "loss": 0.5371,
      "step": 3860
    },
    {
      "epoch": 1.898096992019644,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029430500982318265,
      "loss": 0.5154,
      "step": 3865
    },
    {
      "epoch": 1.9005524861878453,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002942976424361493,
      "loss": 0.5393,
      "step": 3870
    },
    {
      "epoch": 1.9030079803560467,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029429027504911586,
      "loss": 0.5205,
      "step": 3875
    },
    {
      "epoch": 1.905463474524248,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029428290766208247,
      "loss": 0.5425,
      "step": 3880
    },
    {
      "epoch": 1.9079189686924494,
      "grad_norm": 1.5,
      "learning_rate": 0.00029427554027504913,
      "loss": 0.5346,
      "step": 3885
    },
    {
      "epoch": 1.9103744628606507,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002942681728880157,
      "loss": 0.4975,
      "step": 3890
    },
    {
      "epoch": 1.912829957028852,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002942608055009823,
      "loss": 0.5323,
      "step": 3895
    },
    {
      "epoch": 1.9152854511970534,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002942534381139489,
      "loss": 0.5417,
      "step": 3900
    },
    {
      "epoch": 1.9177409453652547,
      "grad_norm": 1.40625,
      "learning_rate": 0.0002942460707269155,
      "loss": 0.5621,
      "step": 3905
    },
    {
      "epoch": 1.920196439533456,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002942387033398821,
      "loss": 0.5074,
      "step": 3910
    },
    {
      "epoch": 1.9226519337016574,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002942313359528487,
      "loss": 0.5192,
      "step": 3915
    },
    {
      "epoch": 1.9251074278698588,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029422396856581526,
      "loss": 0.5285,
      "step": 3920
    },
    {
      "epoch": 1.9275629220380601,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002942166011787819,
      "loss": 0.488,
      "step": 3925
    },
    {
      "epoch": 1.9300184162062615,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002942092337917485,
      "loss": 0.5139,
      "step": 3930
    },
    {
      "epoch": 1.9324739103744628,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002942018664047151,
      "loss": 0.5387,
      "step": 3935
    },
    {
      "epoch": 1.9349294045426642,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029419449901768173,
      "loss": 0.5001,
      "step": 3940
    },
    {
      "epoch": 1.9373848987108655,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002941871316306483,
      "loss": 0.5394,
      "step": 3945
    },
    {
      "epoch": 1.9398403928790668,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002941797642436149,
      "loss": 0.557,
      "step": 3950
    },
    {
      "epoch": 1.9422958870472682,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002941723968565815,
      "loss": 0.5076,
      "step": 3955
    },
    {
      "epoch": 1.9447513812154695,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002941650294695481,
      "loss": 0.5147,
      "step": 3960
    },
    {
      "epoch": 1.9472068753836709,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002941576620825147,
      "loss": 0.5249,
      "step": 3965
    },
    {
      "epoch": 1.9496623695518722,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002941502946954813,
      "loss": 0.5151,
      "step": 3970
    },
    {
      "epoch": 1.9521178637200736,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002941429273084479,
      "loss": 0.5373,
      "step": 3975
    },
    {
      "epoch": 1.954573357888275,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002941355599214145,
      "loss": 0.4851,
      "step": 3980
    },
    {
      "epoch": 1.9570288520564763,
      "grad_norm": 1.5,
      "learning_rate": 0.0002941281925343811,
      "loss": 0.515,
      "step": 3985
    },
    {
      "epoch": 1.9594843462246776,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029412082514734773,
      "loss": 0.5244,
      "step": 3990
    },
    {
      "epoch": 1.961939840392879,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029411345776031434,
      "loss": 0.5516,
      "step": 3995
    },
    {
      "epoch": 1.9643953345610803,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029410609037328094,
      "loss": 0.5075,
      "step": 4000
    },
    {
      "epoch": 1.9668508287292816,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002940987229862475,
      "loss": 0.542,
      "step": 4005
    },
    {
      "epoch": 1.969306322897483,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002940913555992141,
      "loss": 0.5088,
      "step": 4010
    },
    {
      "epoch": 1.9717618170656843,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029408398821218076,
      "loss": 0.5238,
      "step": 4015
    },
    {
      "epoch": 1.9742173112338857,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002940766208251473,
      "loss": 0.525,
      "step": 4020
    },
    {
      "epoch": 1.976672805402087,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002940692534381139,
      "loss": 0.5375,
      "step": 4025
    },
    {
      "epoch": 1.9791282995702886,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002940618860510805,
      "loss": 0.5238,
      "step": 4030
    },
    {
      "epoch": 1.98158379373849,
      "grad_norm": 1.5,
      "learning_rate": 0.0002940545186640471,
      "loss": 0.5236,
      "step": 4035
    },
    {
      "epoch": 1.9840392879066913,
      "grad_norm": 1.71875,
      "learning_rate": 0.00029404715127701373,
      "loss": 0.4866,
      "step": 4040
    },
    {
      "epoch": 1.9864947820748926,
      "grad_norm": 1.6328125,
      "learning_rate": 0.00029403978388998033,
      "loss": 0.545,
      "step": 4045
    },
    {
      "epoch": 1.988950276243094,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029403241650294694,
      "loss": 0.4848,
      "step": 4050
    },
    {
      "epoch": 1.9914057704112953,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029402504911591354,
      "loss": 0.4813,
      "step": 4055
    },
    {
      "epoch": 1.9938612645794966,
      "grad_norm": 1.5,
      "learning_rate": 0.00029401768172888015,
      "loss": 0.5008,
      "step": 4060
    },
    {
      "epoch": 1.996316758747698,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002940103143418467,
      "loss": 0.5092,
      "step": 4065
    },
    {
      "epoch": 1.9987722529158993,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029400294695481336,
      "loss": 0.5837,
      "step": 4070
    },
    {
      "epoch": 2.001227747084101,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029399557956777996,
      "loss": 0.4933,
      "step": 4075
    },
    {
      "epoch": 2.0036832412523022,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002939882121807465,
      "loss": 0.4214,
      "step": 4080
    },
    {
      "epoch": 2.0061387354205036,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002939808447937131,
      "loss": 0.4737,
      "step": 4085
    },
    {
      "epoch": 2.008594229588705,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002939734774066797,
      "loss": 0.4655,
      "step": 4090
    },
    {
      "epoch": 2.0110497237569063,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029396611001964633,
      "loss": 0.456,
      "step": 4095
    },
    {
      "epoch": 2.0135052179251076,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029395874263261294,
      "loss": 0.4772,
      "step": 4100
    },
    {
      "epoch": 2.015960712093309,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00029395137524557954,
      "loss": 0.4494,
      "step": 4105
    },
    {
      "epoch": 2.0184162062615103,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029394400785854615,
      "loss": 0.4463,
      "step": 4110
    },
    {
      "epoch": 2.0208717004297116,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029393664047151275,
      "loss": 0.4843,
      "step": 4115
    },
    {
      "epoch": 2.023327194597913,
      "grad_norm": 1.5,
      "learning_rate": 0.00029392927308447936,
      "loss": 0.4402,
      "step": 4120
    },
    {
      "epoch": 2.0257826887661143,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029392190569744596,
      "loss": 0.4529,
      "step": 4125
    },
    {
      "epoch": 2.0282381829343157,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029391453831041257,
      "loss": 0.4172,
      "step": 4130
    },
    {
      "epoch": 2.030693677102517,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002939071709233791,
      "loss": 0.4565,
      "step": 4135
    },
    {
      "epoch": 2.0331491712707184,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002938998035363457,
      "loss": 0.4731,
      "step": 4140
    },
    {
      "epoch": 2.0356046654389197,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002938924361493124,
      "loss": 0.4507,
      "step": 4145
    },
    {
      "epoch": 2.038060159607121,
      "grad_norm": 1.5,
      "learning_rate": 0.00029388506876227893,
      "loss": 0.4666,
      "step": 4150
    },
    {
      "epoch": 2.0405156537753224,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029387770137524554,
      "loss": 0.4421,
      "step": 4155
    },
    {
      "epoch": 2.0429711479435237,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029387033398821214,
      "loss": 0.4538,
      "step": 4160
    },
    {
      "epoch": 2.045426642111725,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029386296660117875,
      "loss": 0.4843,
      "step": 4165
    },
    {
      "epoch": 2.0478821362799264,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029385559921414535,
      "loss": 0.4728,
      "step": 4170
    },
    {
      "epoch": 2.050337630448128,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029384823182711196,
      "loss": 0.4742,
      "step": 4175
    },
    {
      "epoch": 2.052793124616329,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029384086444007857,
      "loss": 0.4661,
      "step": 4180
    },
    {
      "epoch": 2.0552486187845305,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029383349705304517,
      "loss": 0.4798,
      "step": 4185
    },
    {
      "epoch": 2.057704112952732,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002938261296660118,
      "loss": 0.4517,
      "step": 4190
    },
    {
      "epoch": 2.060159607120933,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002938187622789783,
      "loss": 0.4503,
      "step": 4195
    },
    {
      "epoch": 2.0626151012891345,
      "grad_norm": 1.546875,
      "learning_rate": 0.000293811394891945,
      "loss": 0.4619,
      "step": 4200
    },
    {
      "epoch": 2.065070595457336,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002938040275049116,
      "loss": 0.4451,
      "step": 4205
    },
    {
      "epoch": 2.067526089625537,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029379666011787814,
      "loss": 0.4457,
      "step": 4210
    },
    {
      "epoch": 2.0699815837937385,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002937892927308448,
      "loss": 0.468,
      "step": 4215
    },
    {
      "epoch": 2.07243707796194,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029378192534381135,
      "loss": 0.4713,
      "step": 4220
    },
    {
      "epoch": 2.074892572130141,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029377455795677796,
      "loss": 0.4631,
      "step": 4225
    },
    {
      "epoch": 2.0773480662983426,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029376719056974456,
      "loss": 0.4267,
      "step": 4230
    },
    {
      "epoch": 2.079803560466544,
      "grad_norm": 1.703125,
      "learning_rate": 0.00029375982318271117,
      "loss": 0.4656,
      "step": 4235
    },
    {
      "epoch": 2.0822590546347453,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002937524557956778,
      "loss": 0.4817,
      "step": 4240
    },
    {
      "epoch": 2.0847145488029466,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002937450884086444,
      "loss": 0.4614,
      "step": 4245
    },
    {
      "epoch": 2.087170042971148,
      "grad_norm": 1.546875,
      "learning_rate": 0.000293737721021611,
      "loss": 0.4446,
      "step": 4250
    },
    {
      "epoch": 2.0896255371393493,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002937303536345776,
      "loss": 0.47,
      "step": 4255
    },
    {
      "epoch": 2.0920810313075506,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002937229862475442,
      "loss": 0.511,
      "step": 4260
    },
    {
      "epoch": 2.094536525475752,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029371561886051075,
      "loss": 0.4183,
      "step": 4265
    },
    {
      "epoch": 2.0969920196439533,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002937082514734774,
      "loss": 0.4841,
      "step": 4270
    },
    {
      "epoch": 2.0994475138121547,
      "grad_norm": 1.5234375,
      "learning_rate": 0.000293700884086444,
      "loss": 0.492,
      "step": 4275
    },
    {
      "epoch": 2.101903007980356,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029369351669941056,
      "loss": 0.502,
      "step": 4280
    },
    {
      "epoch": 2.1043585021485574,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029368614931237717,
      "loss": 0.4713,
      "step": 4285
    },
    {
      "epoch": 2.1068139963167587,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029367878192534377,
      "loss": 0.4771,
      "step": 4290
    },
    {
      "epoch": 2.10926949048496,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002936714145383104,
      "loss": 0.4665,
      "step": 4295
    },
    {
      "epoch": 2.1117249846531614,
      "grad_norm": 1.4453125,
      "learning_rate": 0.000293664047151277,
      "loss": 0.4742,
      "step": 4300
    },
    {
      "epoch": 2.1141804788213627,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002936566797642436,
      "loss": 0.4769,
      "step": 4305
    },
    {
      "epoch": 2.116635972989564,
      "grad_norm": 1.796875,
      "learning_rate": 0.0002936493123772102,
      "loss": 0.4787,
      "step": 4310
    },
    {
      "epoch": 2.1190914671577654,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002936419449901768,
      "loss": 0.4472,
      "step": 4315
    },
    {
      "epoch": 2.1215469613259668,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002936345776031434,
      "loss": 0.475,
      "step": 4320
    },
    {
      "epoch": 2.124002455494168,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029362721021611,
      "loss": 0.4475,
      "step": 4325
    },
    {
      "epoch": 2.1264579496623695,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002936198428290766,
      "loss": 0.4985,
      "step": 4330
    },
    {
      "epoch": 2.128913443830571,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002936124754420432,
      "loss": 0.4449,
      "step": 4335
    },
    {
      "epoch": 2.131368937998772,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029360510805500977,
      "loss": 0.4352,
      "step": 4340
    },
    {
      "epoch": 2.1338244321669735,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029359774066797643,
      "loss": 0.4446,
      "step": 4345
    },
    {
      "epoch": 2.136279926335175,
      "grad_norm": 1.4765625,
      "learning_rate": 0.000293590373280943,
      "loss": 0.4507,
      "step": 4350
    },
    {
      "epoch": 2.138735420503376,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002935830058939096,
      "loss": 0.4375,
      "step": 4355
    },
    {
      "epoch": 2.1411909146715775,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002935756385068762,
      "loss": 0.445,
      "step": 4360
    },
    {
      "epoch": 2.143646408839779,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002935682711198428,
      "loss": 0.449,
      "step": 4365
    },
    {
      "epoch": 2.14610190300798,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002935609037328094,
      "loss": 0.4416,
      "step": 4370
    },
    {
      "epoch": 2.1485573971761815,
      "grad_norm": 1.5703125,
      "learning_rate": 0.000293553536345776,
      "loss": 0.4481,
      "step": 4375
    },
    {
      "epoch": 2.151012891344383,
      "grad_norm": 1.71875,
      "learning_rate": 0.0002935461689587426,
      "loss": 0.4934,
      "step": 4380
    },
    {
      "epoch": 2.1534683855125842,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002935388015717092,
      "loss": 0.4615,
      "step": 4385
    },
    {
      "epoch": 2.1559238796807856,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002935314341846758,
      "loss": 0.4869,
      "step": 4390
    },
    {
      "epoch": 2.158379373848987,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029352406679764237,
      "loss": 0.4968,
      "step": 4395
    },
    {
      "epoch": 2.1608348680171883,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029351669941060903,
      "loss": 0.4604,
      "step": 4400
    },
    {
      "epoch": 2.1632903621853896,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029350933202357564,
      "loss": 0.4784,
      "step": 4405
    },
    {
      "epoch": 2.165745856353591,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002935019646365422,
      "loss": 0.4515,
      "step": 4410
    },
    {
      "epoch": 2.1682013505217923,
      "grad_norm": 1.6953125,
      "learning_rate": 0.0002934945972495088,
      "loss": 0.4865,
      "step": 4415
    },
    {
      "epoch": 2.1706568446899936,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029348722986247545,
      "loss": 0.4897,
      "step": 4420
    },
    {
      "epoch": 2.1731123388581954,
      "grad_norm": 1.7265625,
      "learning_rate": 0.000293479862475442,
      "loss": 0.4751,
      "step": 4425
    },
    {
      "epoch": 2.1755678330263963,
      "grad_norm": 1.5,
      "learning_rate": 0.0002934724950884086,
      "loss": 0.4603,
      "step": 4430
    },
    {
      "epoch": 2.178023327194598,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002934651277013752,
      "loss": 0.4106,
      "step": 4435
    },
    {
      "epoch": 2.1804788213627995,
      "grad_norm": 1.5,
      "learning_rate": 0.0002934577603143418,
      "loss": 0.4516,
      "step": 4440
    },
    {
      "epoch": 2.182934315531001,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002934503929273084,
      "loss": 0.443,
      "step": 4445
    },
    {
      "epoch": 2.185389809699202,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029344302554027503,
      "loss": 0.4569,
      "step": 4450
    },
    {
      "epoch": 2.1878453038674035,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029343565815324163,
      "loss": 0.4736,
      "step": 4455
    },
    {
      "epoch": 2.190300798035605,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029342829076620824,
      "loss": 0.4525,
      "step": 4460
    },
    {
      "epoch": 2.192756292203806,
      "grad_norm": 1.5,
      "learning_rate": 0.00029342092337917485,
      "loss": 0.4709,
      "step": 4465
    },
    {
      "epoch": 2.1952117863720075,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002934135559921414,
      "loss": 0.463,
      "step": 4470
    },
    {
      "epoch": 2.197667280540209,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029340618860510806,
      "loss": 0.4935,
      "step": 4475
    },
    {
      "epoch": 2.2001227747084102,
      "grad_norm": 1.7109375,
      "learning_rate": 0.0002933988212180746,
      "loss": 0.4422,
      "step": 4480
    },
    {
      "epoch": 2.2025782688766116,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002933914538310412,
      "loss": 0.4635,
      "step": 4485
    },
    {
      "epoch": 2.205033763044813,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029338408644400787,
      "loss": 0.4464,
      "step": 4490
    },
    {
      "epoch": 2.2074892572130143,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002933767190569744,
      "loss": 0.4316,
      "step": 4495
    },
    {
      "epoch": 2.2099447513812156,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029336935166994103,
      "loss": 0.4802,
      "step": 4500
    },
    {
      "epoch": 2.212400245549417,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029336198428290763,
      "loss": 0.4674,
      "step": 4505
    },
    {
      "epoch": 2.2148557397176183,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029335461689587424,
      "loss": 0.4592,
      "step": 4510
    },
    {
      "epoch": 2.2173112338858196,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029334724950884084,
      "loss": 0.4506,
      "step": 4515
    },
    {
      "epoch": 2.219766728054021,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029333988212180745,
      "loss": 0.431,
      "step": 4520
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029333251473477405,
      "loss": 0.4615,
      "step": 4525
    },
    {
      "epoch": 2.2246777163904237,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029332514734774066,
      "loss": 0.452,
      "step": 4530
    },
    {
      "epoch": 2.227133210558625,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029331777996070726,
      "loss": 0.4755,
      "step": 4535
    },
    {
      "epoch": 2.2295887047268264,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002933104125736738,
      "loss": 0.4974,
      "step": 4540
    },
    {
      "epoch": 2.2320441988950277,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002933030451866405,
      "loss": 0.4633,
      "step": 4545
    },
    {
      "epoch": 2.234499693063229,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002932956777996071,
      "loss": 0.4406,
      "step": 4550
    },
    {
      "epoch": 2.2369551872314304,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029328831041257363,
      "loss": 0.4507,
      "step": 4555
    },
    {
      "epoch": 2.2394106813996317,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029328094302554024,
      "loss": 0.4564,
      "step": 4560
    },
    {
      "epoch": 2.241866175567833,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029327357563850684,
      "loss": 0.443,
      "step": 4565
    },
    {
      "epoch": 2.2443216697360344,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029326620825147345,
      "loss": 0.483,
      "step": 4570
    },
    {
      "epoch": 2.2467771639042358,
      "grad_norm": 1.5,
      "learning_rate": 0.00029325884086444005,
      "loss": 0.4825,
      "step": 4575
    },
    {
      "epoch": 2.249232658072437,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029325147347740666,
      "loss": 0.4761,
      "step": 4580
    },
    {
      "epoch": 2.2516881522406385,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029324410609037326,
      "loss": 0.4968,
      "step": 4585
    },
    {
      "epoch": 2.25414364640884,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029323673870333987,
      "loss": 0.4404,
      "step": 4590
    },
    {
      "epoch": 2.256599140577041,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029322937131630647,
      "loss": 0.4492,
      "step": 4595
    },
    {
      "epoch": 2.2590546347452425,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002932220039292731,
      "loss": 0.4627,
      "step": 4600
    },
    {
      "epoch": 2.261510128913444,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002932146365422397,
      "loss": 0.45,
      "step": 4605
    },
    {
      "epoch": 2.263965623081645,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029320726915520623,
      "loss": 0.4891,
      "step": 4610
    },
    {
      "epoch": 2.2664211172498465,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029319990176817284,
      "loss": 0.4819,
      "step": 4615
    },
    {
      "epoch": 2.268876611418048,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002931925343811395,
      "loss": 0.4413,
      "step": 4620
    },
    {
      "epoch": 2.271332105586249,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029318516699410605,
      "loss": 0.4598,
      "step": 4625
    },
    {
      "epoch": 2.2737875997544506,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029317779960707265,
      "loss": 0.4437,
      "step": 4630
    },
    {
      "epoch": 2.276243093922652,
      "grad_norm": 1.3515625,
      "learning_rate": 0.00029317043222003926,
      "loss": 0.4699,
      "step": 4635
    },
    {
      "epoch": 2.2786985880908532,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029316306483300586,
      "loss": 0.4559,
      "step": 4640
    },
    {
      "epoch": 2.2811540822590546,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029315569744597247,
      "loss": 0.4395,
      "step": 4645
    },
    {
      "epoch": 2.283609576427256,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002931483300589391,
      "loss": 0.458,
      "step": 4650
    },
    {
      "epoch": 2.2860650705954573,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002931409626719057,
      "loss": 0.4647,
      "step": 4655
    },
    {
      "epoch": 2.2885205647636586,
      "grad_norm": 1.625,
      "learning_rate": 0.0002931335952848723,
      "loss": 0.4652,
      "step": 4660
    },
    {
      "epoch": 2.29097605893186,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002931262278978389,
      "loss": 0.4673,
      "step": 4665
    },
    {
      "epoch": 2.2934315531000613,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00029311886051080544,
      "loss": 0.4557,
      "step": 4670
    },
    {
      "epoch": 2.2958870472682626,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002931114931237721,
      "loss": 0.4717,
      "step": 4675
    },
    {
      "epoch": 2.298342541436464,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002931041257367387,
      "loss": 0.448,
      "step": 4680
    },
    {
      "epoch": 2.3007980356046653,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029309675834970526,
      "loss": 0.4436,
      "step": 4685
    },
    {
      "epoch": 2.3032535297728667,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029308939096267186,
      "loss": 0.4561,
      "step": 4690
    },
    {
      "epoch": 2.305709023941068,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029308202357563847,
      "loss": 0.4755,
      "step": 4695
    },
    {
      "epoch": 2.3081645181092694,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002930746561886051,
      "loss": 0.4214,
      "step": 4700
    },
    {
      "epoch": 2.3106200122774707,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002930672888015717,
      "loss": 0.4696,
      "step": 4705
    },
    {
      "epoch": 2.313075506445672,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002930599214145383,
      "loss": 0.4863,
      "step": 4710
    },
    {
      "epoch": 2.3155310006138734,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002930525540275049,
      "loss": 0.47,
      "step": 4715
    },
    {
      "epoch": 2.3179864947820747,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002930451866404715,
      "loss": 0.4192,
      "step": 4720
    },
    {
      "epoch": 2.320441988950276,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002930378192534381,
      "loss": 0.4936,
      "step": 4725
    },
    {
      "epoch": 2.3228974831184774,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002930304518664047,
      "loss": 0.4246,
      "step": 4730
    },
    {
      "epoch": 2.325352977286679,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002930230844793713,
      "loss": 0.4609,
      "step": 4735
    },
    {
      "epoch": 2.32780847145488,
      "grad_norm": 1.3828125,
      "learning_rate": 0.00029301571709233786,
      "loss": 0.4521,
      "step": 4740
    },
    {
      "epoch": 2.3302639656230815,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029300834970530447,
      "loss": 0.4479,
      "step": 4745
    },
    {
      "epoch": 2.332719459791283,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002930009823182711,
      "loss": 0.4361,
      "step": 4750
    },
    {
      "epoch": 2.335174953959484,
      "grad_norm": 1.34375,
      "learning_rate": 0.0002929936149312377,
      "loss": 0.4704,
      "step": 4755
    },
    {
      "epoch": 2.337630448127686,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002929862475442043,
      "loss": 0.4392,
      "step": 4760
    },
    {
      "epoch": 2.340085942295887,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029297888015717094,
      "loss": 0.4191,
      "step": 4765
    },
    {
      "epoch": 2.3425414364640886,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002929715127701375,
      "loss": 0.484,
      "step": 4770
    },
    {
      "epoch": 2.3449969306322895,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002929641453831041,
      "loss": 0.4196,
      "step": 4775
    },
    {
      "epoch": 2.3474524248004913,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002929567779960707,
      "loss": 0.4573,
      "step": 4780
    },
    {
      "epoch": 2.349907918968692,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002929494106090373,
      "loss": 0.4953,
      "step": 4785
    },
    {
      "epoch": 2.352363413136894,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002929420432220039,
      "loss": 0.4577,
      "step": 4790
    },
    {
      "epoch": 2.354818907305095,
      "grad_norm": 1.5,
      "learning_rate": 0.0002929346758349705,
      "loss": 0.4514,
      "step": 4795
    },
    {
      "epoch": 2.3572744014732967,
      "grad_norm": 1.6328125,
      "learning_rate": 0.00029292730844793707,
      "loss": 0.4948,
      "step": 4800
    },
    {
      "epoch": 2.359729895641498,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029291994106090373,
      "loss": 0.4565,
      "step": 4805
    },
    {
      "epoch": 2.3621853898096994,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029291257367387033,
      "loss": 0.4562,
      "step": 4810
    },
    {
      "epoch": 2.3646408839779007,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002929052062868369,
      "loss": 0.4629,
      "step": 4815
    },
    {
      "epoch": 2.367096378146102,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002928978388998035,
      "loss": 0.455,
      "step": 4820
    },
    {
      "epoch": 2.3695518723143034,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002928904715127701,
      "loss": 0.4238,
      "step": 4825
    },
    {
      "epoch": 2.3720073664825048,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002928831041257367,
      "loss": 0.4736,
      "step": 4830
    },
    {
      "epoch": 2.374462860650706,
      "grad_norm": 1.6953125,
      "learning_rate": 0.0002928757367387033,
      "loss": 0.4489,
      "step": 4835
    },
    {
      "epoch": 2.3769183548189075,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002928683693516699,
      "loss": 0.4578,
      "step": 4840
    },
    {
      "epoch": 2.379373848987109,
      "grad_norm": 1.40625,
      "learning_rate": 0.0002928610019646365,
      "loss": 0.4851,
      "step": 4845
    },
    {
      "epoch": 2.38182934315531,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002928536345776031,
      "loss": 0.4726,
      "step": 4850
    },
    {
      "epoch": 2.3842848373235115,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002928462671905697,
      "loss": 0.4472,
      "step": 4855
    },
    {
      "epoch": 2.386740331491713,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029283889980353633,
      "loss": 0.4377,
      "step": 4860
    },
    {
      "epoch": 2.389195825659914,
      "grad_norm": 1.625,
      "learning_rate": 0.00029283153241650294,
      "loss": 0.4735,
      "step": 4865
    },
    {
      "epoch": 2.3916513198281155,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029282416502946954,
      "loss": 0.4453,
      "step": 4870
    },
    {
      "epoch": 2.394106813996317,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002928167976424361,
      "loss": 0.4368,
      "step": 4875
    },
    {
      "epoch": 2.396562308164518,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029280943025540275,
      "loss": 0.444,
      "step": 4880
    },
    {
      "epoch": 2.3990178023327196,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002928020628683693,
      "loss": 0.4732,
      "step": 4885
    },
    {
      "epoch": 2.401473296500921,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002927946954813359,
      "loss": 0.426,
      "step": 4890
    },
    {
      "epoch": 2.4039287906691222,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00029278732809430257,
      "loss": 0.4228,
      "step": 4895
    },
    {
      "epoch": 2.4063842848373236,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0002927799607072691,
      "loss": 0.4277,
      "step": 4900
    },
    {
      "epoch": 2.408839779005525,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002927725933202357,
      "loss": 0.4493,
      "step": 4905
    },
    {
      "epoch": 2.4112952731737263,
      "grad_norm": 1.5,
      "learning_rate": 0.00029276522593320233,
      "loss": 0.4538,
      "step": 4910
    },
    {
      "epoch": 2.4137507673419276,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029275785854616893,
      "loss": 0.4445,
      "step": 4915
    },
    {
      "epoch": 2.416206261510129,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029275049115913554,
      "loss": 0.4338,
      "step": 4920
    },
    {
      "epoch": 2.4186617556783303,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029274312377210214,
      "loss": 0.4695,
      "step": 4925
    },
    {
      "epoch": 2.4211172498465316,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002927357563850687,
      "loss": 0.4451,
      "step": 4930
    },
    {
      "epoch": 2.423572744014733,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029272838899803536,
      "loss": 0.4363,
      "step": 4935
    },
    {
      "epoch": 2.4260282381829343,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029272102161100196,
      "loss": 0.4737,
      "step": 4940
    },
    {
      "epoch": 2.4284837323511357,
      "grad_norm": 1.5,
      "learning_rate": 0.0002927136542239685,
      "loss": 0.4744,
      "step": 4945
    },
    {
      "epoch": 2.430939226519337,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029270628683693517,
      "loss": 0.4771,
      "step": 4950
    },
    {
      "epoch": 2.4333947206875384,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002926989194499017,
      "loss": 0.4538,
      "step": 4955
    },
    {
      "epoch": 2.4358502148557397,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002926915520628683,
      "loss": 0.4402,
      "step": 4960
    },
    {
      "epoch": 2.438305709023941,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029268418467583493,
      "loss": 0.4659,
      "step": 4965
    },
    {
      "epoch": 2.4407612031921424,
      "grad_norm": 1.671875,
      "learning_rate": 0.00029267681728880154,
      "loss": 0.4469,
      "step": 4970
    },
    {
      "epoch": 2.4432166973603437,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029266944990176814,
      "loss": 0.462,
      "step": 4975
    },
    {
      "epoch": 2.445672191528545,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029266208251473475,
      "loss": 0.427,
      "step": 4980
    },
    {
      "epoch": 2.4481276856967464,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00029265471512770135,
      "loss": 0.4593,
      "step": 4985
    },
    {
      "epoch": 2.450583179864948,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029264734774066796,
      "loss": 0.426,
      "step": 4990
    },
    {
      "epoch": 2.453038674033149,
      "grad_norm": 1.40625,
      "learning_rate": 0.00029263998035363456,
      "loss": 0.4339,
      "step": 4995
    },
    {
      "epoch": 2.4554941682013505,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029263261296660117,
      "loss": 0.4572,
      "step": 5000
    },
    {
      "epoch": 2.457949662369552,
      "grad_norm": 1.75,
      "learning_rate": 0.0002926252455795678,
      "loss": 0.4633,
      "step": 5005
    },
    {
      "epoch": 2.460405156537753,
      "grad_norm": 1.5,
      "learning_rate": 0.0002926178781925344,
      "loss": 0.4502,
      "step": 5010
    },
    {
      "epoch": 2.4628606507059545,
      "grad_norm": 1.7109375,
      "learning_rate": 0.00029261051080550093,
      "loss": 0.5011,
      "step": 5015
    },
    {
      "epoch": 2.465316144874156,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029260314341846754,
      "loss": 0.4336,
      "step": 5020
    },
    {
      "epoch": 2.467771639042357,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002925957760314342,
      "loss": 0.4572,
      "step": 5025
    },
    {
      "epoch": 2.4702271332105585,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029258840864440075,
      "loss": 0.4523,
      "step": 5030
    },
    {
      "epoch": 2.47268262737876,
      "grad_norm": 1.65625,
      "learning_rate": 0.00029258104125736735,
      "loss": 0.4322,
      "step": 5035
    },
    {
      "epoch": 2.4751381215469612,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029257367387033396,
      "loss": 0.4604,
      "step": 5040
    },
    {
      "epoch": 2.4775936157151626,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029256630648330056,
      "loss": 0.456,
      "step": 5045
    },
    {
      "epoch": 2.480049109883364,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029255893909626717,
      "loss": 0.4737,
      "step": 5050
    },
    {
      "epoch": 2.4825046040515653,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029255157170923377,
      "loss": 0.4458,
      "step": 5055
    },
    {
      "epoch": 2.4849600982197666,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002925442043222004,
      "loss": 0.4755,
      "step": 5060
    },
    {
      "epoch": 2.487415592387968,
      "grad_norm": 1.46875,
      "learning_rate": 0.000292536836935167,
      "loss": 0.4559,
      "step": 5065
    },
    {
      "epoch": 2.4898710865561693,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002925294695481336,
      "loss": 0.4361,
      "step": 5070
    },
    {
      "epoch": 2.4923265807243706,
      "grad_norm": 1.65625,
      "learning_rate": 0.00029252210216110014,
      "loss": 0.4526,
      "step": 5075
    },
    {
      "epoch": 2.494782074892572,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002925147347740668,
      "loss": 0.4376,
      "step": 5080
    },
    {
      "epoch": 2.4972375690607733,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029250736738703335,
      "loss": 0.4316,
      "step": 5085
    },
    {
      "epoch": 2.4996930632289747,
      "grad_norm": 1.625,
      "learning_rate": 0.00029249999999999995,
      "loss": 0.4438,
      "step": 5090
    },
    {
      "epoch": 2.5021485573971765,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029249263261296656,
      "loss": 0.442,
      "step": 5095
    },
    {
      "epoch": 2.5046040515653774,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029248526522593316,
      "loss": 0.4684,
      "step": 5100
    },
    {
      "epoch": 2.507059545733579,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00029247789783889977,
      "loss": 0.4556,
      "step": 5105
    },
    {
      "epoch": 2.50951503990178,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002924705304518664,
      "loss": 0.4767,
      "step": 5110
    },
    {
      "epoch": 2.511970534069982,
      "grad_norm": 1.5546875,
      "learning_rate": 0.000292463163064833,
      "loss": 0.4287,
      "step": 5115
    },
    {
      "epoch": 2.5144260282381827,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002924557956777996,
      "loss": 0.4531,
      "step": 5120
    },
    {
      "epoch": 2.5168815224063845,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002924484282907662,
      "loss": 0.4392,
      "step": 5125
    },
    {
      "epoch": 2.5193370165745854,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002924410609037328,
      "loss": 0.4684,
      "step": 5130
    },
    {
      "epoch": 2.521792510742787,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002924336935166994,
      "loss": 0.438,
      "step": 5135
    },
    {
      "epoch": 2.524248004910988,
      "grad_norm": 1.625,
      "learning_rate": 0.000292426326129666,
      "loss": 0.4458,
      "step": 5140
    },
    {
      "epoch": 2.52670349907919,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029241895874263256,
      "loss": 0.4481,
      "step": 5145
    },
    {
      "epoch": 2.529158993247391,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029241159135559916,
      "loss": 0.4416,
      "step": 5150
    },
    {
      "epoch": 2.5316144874155926,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002924042239685658,
      "loss": 0.442,
      "step": 5155
    },
    {
      "epoch": 2.5340699815837935,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029239685658153237,
      "loss": 0.4592,
      "step": 5160
    },
    {
      "epoch": 2.5365254757519953,
      "grad_norm": 1.53125,
      "learning_rate": 0.000292389489194499,
      "loss": 0.4697,
      "step": 5165
    },
    {
      "epoch": 2.538980969920196,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002923821218074656,
      "loss": 0.4322,
      "step": 5170
    },
    {
      "epoch": 2.541436464088398,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002923747544204322,
      "loss": 0.4307,
      "step": 5175
    },
    {
      "epoch": 2.5438919582565993,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002923673870333988,
      "loss": 0.4741,
      "step": 5180
    },
    {
      "epoch": 2.5463474524248007,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002923600196463654,
      "loss": 0.4607,
      "step": 5185
    },
    {
      "epoch": 2.548802946593002,
      "grad_norm": 1.4765625,
      "learning_rate": 0.000292352652259332,
      "loss": 0.4967,
      "step": 5190
    },
    {
      "epoch": 2.5512584407612033,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002923452848722986,
      "loss": 0.4725,
      "step": 5195
    },
    {
      "epoch": 2.5537139349294047,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002923379174852652,
      "loss": 0.4167,
      "step": 5200
    },
    {
      "epoch": 2.556169429097606,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029233055009823177,
      "loss": 0.4454,
      "step": 5205
    },
    {
      "epoch": 2.5586249232658074,
      "grad_norm": 1.375,
      "learning_rate": 0.0002923231827111984,
      "loss": 0.4515,
      "step": 5210
    },
    {
      "epoch": 2.5610804174340087,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029231581532416503,
      "loss": 0.4605,
      "step": 5215
    },
    {
      "epoch": 2.56353591160221,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002923084479371316,
      "loss": 0.4295,
      "step": 5220
    },
    {
      "epoch": 2.5659914057704114,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00029230108055009824,
      "loss": 0.4813,
      "step": 5225
    },
    {
      "epoch": 2.5684468999386127,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002922937131630648,
      "loss": 0.4229,
      "step": 5230
    },
    {
      "epoch": 2.570902394106814,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002922863457760314,
      "loss": 0.427,
      "step": 5235
    },
    {
      "epoch": 2.5733578882750154,
      "grad_norm": 1.4453125,
      "learning_rate": 0.000292278978388998,
      "loss": 0.436,
      "step": 5240
    },
    {
      "epoch": 2.575813382443217,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002922716110019646,
      "loss": 0.423,
      "step": 5245
    },
    {
      "epoch": 2.578268876611418,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002922642436149312,
      "loss": 0.4673,
      "step": 5250
    },
    {
      "epoch": 2.5807243707796195,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002922568762278978,
      "loss": 0.408,
      "step": 5255
    },
    {
      "epoch": 2.583179864947821,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002922495088408644,
      "loss": 0.442,
      "step": 5260
    },
    {
      "epoch": 2.585635359116022,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029224214145383103,
      "loss": 0.4587,
      "step": 5265
    },
    {
      "epoch": 2.5880908532842235,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029223477406679763,
      "loss": 0.4497,
      "step": 5270
    },
    {
      "epoch": 2.590546347452425,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002922274066797642,
      "loss": 0.4557,
      "step": 5275
    },
    {
      "epoch": 2.593001841620626,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00029222003929273084,
      "loss": 0.4598,
      "step": 5280
    },
    {
      "epoch": 2.5954573357888275,
      "grad_norm": 1.703125,
      "learning_rate": 0.00029221267190569745,
      "loss": 0.415,
      "step": 5285
    },
    {
      "epoch": 2.597912829957029,
      "grad_norm": 1.5234375,
      "learning_rate": 0.000292205304518664,
      "loss": 0.4339,
      "step": 5290
    },
    {
      "epoch": 2.6003683241252302,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002921979371316306,
      "loss": 0.4311,
      "step": 5295
    },
    {
      "epoch": 2.6028238182934316,
      "grad_norm": 1.7109375,
      "learning_rate": 0.0002921905697445972,
      "loss": 0.4663,
      "step": 5300
    },
    {
      "epoch": 2.605279312461633,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002921832023575638,
      "loss": 0.4488,
      "step": 5305
    },
    {
      "epoch": 2.6077348066298343,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002921758349705304,
      "loss": 0.411,
      "step": 5310
    },
    {
      "epoch": 2.6101903007980356,
      "grad_norm": 1.6640625,
      "learning_rate": 0.000292168467583497,
      "loss": 0.457,
      "step": 5315
    },
    {
      "epoch": 2.612645794966237,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029216110019646363,
      "loss": 0.4389,
      "step": 5320
    },
    {
      "epoch": 2.6151012891344383,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029215373280943024,
      "loss": 0.4394,
      "step": 5325
    },
    {
      "epoch": 2.6175567833026396,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029214636542239684,
      "loss": 0.4374,
      "step": 5330
    },
    {
      "epoch": 2.620012277470841,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029213899803536345,
      "loss": 0.4361,
      "step": 5335
    },
    {
      "epoch": 2.6224677716390423,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029213163064833005,
      "loss": 0.462,
      "step": 5340
    },
    {
      "epoch": 2.6249232658072437,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029212426326129666,
      "loss": 0.4506,
      "step": 5345
    },
    {
      "epoch": 2.627378759975445,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002921168958742632,
      "loss": 0.4484,
      "step": 5350
    },
    {
      "epoch": 2.6298342541436464,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029210952848722987,
      "loss": 0.4183,
      "step": 5355
    },
    {
      "epoch": 2.6322897483118477,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002921021611001964,
      "loss": 0.4208,
      "step": 5360
    },
    {
      "epoch": 2.634745242480049,
      "grad_norm": 1.53125,
      "learning_rate": 0.000292094793713163,
      "loss": 0.4571,
      "step": 5365
    },
    {
      "epoch": 2.6372007366482504,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029208742632612963,
      "loss": 0.459,
      "step": 5370
    },
    {
      "epoch": 2.6396562308164517,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029208005893909623,
      "loss": 0.4189,
      "step": 5375
    },
    {
      "epoch": 2.642111724984653,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029207269155206284,
      "loss": 0.4171,
      "step": 5380
    },
    {
      "epoch": 2.6445672191528544,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029206532416502944,
      "loss": 0.4556,
      "step": 5385
    },
    {
      "epoch": 2.6470227133210558,
      "grad_norm": 1.6328125,
      "learning_rate": 0.00029205795677799605,
      "loss": 0.4608,
      "step": 5390
    },
    {
      "epoch": 2.649478207489257,
      "grad_norm": 1.625,
      "learning_rate": 0.00029205058939096265,
      "loss": 0.4683,
      "step": 5395
    },
    {
      "epoch": 2.6519337016574585,
      "grad_norm": 1.65625,
      "learning_rate": 0.00029204322200392926,
      "loss": 0.4932,
      "step": 5400
    },
    {
      "epoch": 2.65438919582566,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002920358546168958,
      "loss": 0.4294,
      "step": 5405
    },
    {
      "epoch": 2.656844689993861,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029202848722986247,
      "loss": 0.4236,
      "step": 5410
    },
    {
      "epoch": 2.6593001841620625,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002920211198428291,
      "loss": 0.4341,
      "step": 5415
    },
    {
      "epoch": 2.661755678330264,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002920137524557956,
      "loss": 0.4501,
      "step": 5420
    },
    {
      "epoch": 2.664211172498465,
      "grad_norm": 1.8515625,
      "learning_rate": 0.00029200638506876223,
      "loss": 0.4293,
      "step": 5425
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5,
      "learning_rate": 0.00029199901768172884,
      "loss": 0.4317,
      "step": 5430
    },
    {
      "epoch": 2.669122160834868,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029199165029469544,
      "loss": 0.3968,
      "step": 5435
    },
    {
      "epoch": 2.671577655003069,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00029198428290766205,
      "loss": 0.4562,
      "step": 5440
    },
    {
      "epoch": 2.6740331491712706,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029197691552062865,
      "loss": 0.4609,
      "step": 5445
    },
    {
      "epoch": 2.6764886433394723,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029196954813359526,
      "loss": 0.4347,
      "step": 5450
    },
    {
      "epoch": 2.6789441375076732,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029196218074656186,
      "loss": 0.3997,
      "step": 5455
    },
    {
      "epoch": 2.681399631675875,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029195481335952847,
      "loss": 0.4056,
      "step": 5460
    },
    {
      "epoch": 2.683855125844076,
      "grad_norm": 1.3671875,
      "learning_rate": 0.0002919474459724951,
      "loss": 0.4239,
      "step": 5465
    },
    {
      "epoch": 2.6863106200122777,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002919400785854617,
      "loss": 0.4094,
      "step": 5470
    },
    {
      "epoch": 2.6887661141804786,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0002919327111984283,
      "loss": 0.4588,
      "step": 5475
    },
    {
      "epoch": 2.6912216083486804,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029192534381139483,
      "loss": 0.4598,
      "step": 5480
    },
    {
      "epoch": 2.6936771025168813,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002919179764243615,
      "loss": 0.4313,
      "step": 5485
    },
    {
      "epoch": 2.696132596685083,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029191060903732805,
      "loss": 0.4383,
      "step": 5490
    },
    {
      "epoch": 2.698588090853284,
      "grad_norm": 1.6875,
      "learning_rate": 0.00029190324165029465,
      "loss": 0.4682,
      "step": 5495
    },
    {
      "epoch": 2.701043585021486,
      "grad_norm": 1.6875,
      "learning_rate": 0.0002918958742632613,
      "loss": 0.4556,
      "step": 5500
    },
    {
      "epoch": 2.7034990791896867,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00029188850687622786,
      "loss": 0.4523,
      "step": 5505
    },
    {
      "epoch": 2.7059545733578885,
      "grad_norm": 1.5,
      "learning_rate": 0.00029188113948919447,
      "loss": 0.4282,
      "step": 5510
    },
    {
      "epoch": 2.7084100675260894,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029187377210216107,
      "loss": 0.4405,
      "step": 5515
    },
    {
      "epoch": 2.710865561694291,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002918664047151277,
      "loss": 0.4069,
      "step": 5520
    },
    {
      "epoch": 2.713321055862492,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002918590373280943,
      "loss": 0.4466,
      "step": 5525
    },
    {
      "epoch": 2.715776550030694,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002918516699410609,
      "loss": 0.4226,
      "step": 5530
    },
    {
      "epoch": 2.718232044198895,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029184430255402744,
      "loss": 0.4395,
      "step": 5535
    },
    {
      "epoch": 2.7206875383670965,
      "grad_norm": 1.734375,
      "learning_rate": 0.0002918369351669941,
      "loss": 0.4704,
      "step": 5540
    },
    {
      "epoch": 2.723143032535298,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002918295677799607,
      "loss": 0.3963,
      "step": 5545
    },
    {
      "epoch": 2.7255985267034992,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029182220039292725,
      "loss": 0.4402,
      "step": 5550
    },
    {
      "epoch": 2.7280540208717006,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002918148330058939,
      "loss": 0.4005,
      "step": 5555
    },
    {
      "epoch": 2.730509515039902,
      "grad_norm": 1.71875,
      "learning_rate": 0.0002918074656188605,
      "loss": 0.4492,
      "step": 5560
    },
    {
      "epoch": 2.7329650092081033,
      "grad_norm": 1.5,
      "learning_rate": 0.00029180009823182707,
      "loss": 0.4101,
      "step": 5565
    },
    {
      "epoch": 2.7354205033763046,
      "grad_norm": 1.5,
      "learning_rate": 0.0002917927308447937,
      "loss": 0.4086,
      "step": 5570
    },
    {
      "epoch": 2.737875997544506,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002917853634577603,
      "loss": 0.4262,
      "step": 5575
    },
    {
      "epoch": 2.7403314917127073,
      "grad_norm": 1.5,
      "learning_rate": 0.0002917779960707269,
      "loss": 0.428,
      "step": 5580
    },
    {
      "epoch": 2.7427869858809086,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002917706286836935,
      "loss": 0.4199,
      "step": 5585
    },
    {
      "epoch": 2.74524248004911,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002917632612966601,
      "loss": 0.4274,
      "step": 5590
    },
    {
      "epoch": 2.7476979742173113,
      "grad_norm": 1.625,
      "learning_rate": 0.0002917558939096267,
      "loss": 0.4223,
      "step": 5595
    },
    {
      "epoch": 2.7501534683855127,
      "grad_norm": 1.7265625,
      "learning_rate": 0.0002917485265225933,
      "loss": 0.4254,
      "step": 5600
    },
    {
      "epoch": 2.752608962553714,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002917411591355599,
      "loss": 0.4313,
      "step": 5605
    },
    {
      "epoch": 2.7550644567219154,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002917337917485265,
      "loss": 0.3859,
      "step": 5610
    },
    {
      "epoch": 2.7575199508901167,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002917264243614931,
      "loss": 0.4667,
      "step": 5615
    },
    {
      "epoch": 2.759975445058318,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029171905697445967,
      "loss": 0.4171,
      "step": 5620
    },
    {
      "epoch": 2.7624309392265194,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002917116895874263,
      "loss": 0.4612,
      "step": 5625
    },
    {
      "epoch": 2.7648864333947207,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029170432220039294,
      "loss": 0.4049,
      "step": 5630
    },
    {
      "epoch": 2.767341927562922,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002916969548133595,
      "loss": 0.4247,
      "step": 5635
    },
    {
      "epoch": 2.7697974217311234,
      "grad_norm": 1.421875,
      "learning_rate": 0.0002916895874263261,
      "loss": 0.4204,
      "step": 5640
    },
    {
      "epoch": 2.7722529158993248,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002916822200392927,
      "loss": 0.4477,
      "step": 5645
    },
    {
      "epoch": 2.774708410067526,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002916748526522593,
      "loss": 0.4273,
      "step": 5650
    },
    {
      "epoch": 2.7771639042357275,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002916674852652259,
      "loss": 0.4455,
      "step": 5655
    },
    {
      "epoch": 2.779619398403929,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002916601178781925,
      "loss": 0.4208,
      "step": 5660
    },
    {
      "epoch": 2.78207489257213,
      "grad_norm": 1.578125,
      "learning_rate": 0.0002916527504911591,
      "loss": 0.4287,
      "step": 5665
    },
    {
      "epoch": 2.7845303867403315,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002916453831041257,
      "loss": 0.4287,
      "step": 5670
    },
    {
      "epoch": 2.786985880908533,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029163801571709233,
      "loss": 0.4281,
      "step": 5675
    },
    {
      "epoch": 2.789441375076734,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002916306483300589,
      "loss": 0.4411,
      "step": 5680
    },
    {
      "epoch": 2.7918968692449355,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029162328094302554,
      "loss": 0.4196,
      "step": 5685
    },
    {
      "epoch": 2.794352363413137,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029161591355599215,
      "loss": 0.4326,
      "step": 5690
    },
    {
      "epoch": 2.796807857581338,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002916085461689587,
      "loss": 0.4787,
      "step": 5695
    },
    {
      "epoch": 2.7992633517495396,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002916011787819253,
      "loss": 0.4171,
      "step": 5700
    },
    {
      "epoch": 2.801718845917741,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002915938113948919,
      "loss": 0.4341,
      "step": 5705
    },
    {
      "epoch": 2.8041743400859422,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002915864440078585,
      "loss": 0.4433,
      "step": 5710
    },
    {
      "epoch": 2.8066298342541436,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002915790766208251,
      "loss": 0.4337,
      "step": 5715
    },
    {
      "epoch": 2.809085328422345,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002915717092337917,
      "loss": 0.4147,
      "step": 5720
    },
    {
      "epoch": 2.8115408225905463,
      "grad_norm": 1.765625,
      "learning_rate": 0.00029156434184675833,
      "loss": 0.4622,
      "step": 5725
    },
    {
      "epoch": 2.8139963167587476,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00029155697445972493,
      "loss": 0.3897,
      "step": 5730
    },
    {
      "epoch": 2.816451810926949,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029154960707269154,
      "loss": 0.4031,
      "step": 5735
    },
    {
      "epoch": 2.8189073050951503,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029154223968565814,
      "loss": 0.4394,
      "step": 5740
    },
    {
      "epoch": 2.8213627992633517,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029153487229862475,
      "loss": 0.4395,
      "step": 5745
    },
    {
      "epoch": 2.823818293431553,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002915275049115913,
      "loss": 0.4437,
      "step": 5750
    },
    {
      "epoch": 2.8262737875997543,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002915201375245579,
      "loss": 0.4362,
      "step": 5755
    },
    {
      "epoch": 2.8287292817679557,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029151277013752456,
      "loss": 0.4822,
      "step": 5760
    },
    {
      "epoch": 2.831184775936157,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002915054027504911,
      "loss": 0.3992,
      "step": 5765
    },
    {
      "epoch": 2.8336402701043584,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002914980353634577,
      "loss": 0.4508,
      "step": 5770
    },
    {
      "epoch": 2.8360957642725597,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002914906679764244,
      "loss": 0.4225,
      "step": 5775
    },
    {
      "epoch": 2.838551258440761,
      "grad_norm": 1.625,
      "learning_rate": 0.00029148330058939093,
      "loss": 0.4034,
      "step": 5780
    },
    {
      "epoch": 2.8410067526089624,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029147593320235754,
      "loss": 0.399,
      "step": 5785
    },
    {
      "epoch": 2.8434622467771637,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029146856581532414,
      "loss": 0.4005,
      "step": 5790
    },
    {
      "epoch": 2.845917740945365,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029146119842829075,
      "loss": 0.4274,
      "step": 5795
    },
    {
      "epoch": 2.8483732351135664,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029145383104125735,
      "loss": 0.4133,
      "step": 5800
    },
    {
      "epoch": 2.8508287292817682,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029144646365422396,
      "loss": 0.4262,
      "step": 5805
    },
    {
      "epoch": 2.853284223449969,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002914390962671905,
      "loss": 0.4139,
      "step": 5810
    },
    {
      "epoch": 2.855739717618171,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029143172888015717,
      "loss": 0.4198,
      "step": 5815
    },
    {
      "epoch": 2.858195211786372,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029142436149312377,
      "loss": 0.4071,
      "step": 5820
    },
    {
      "epoch": 2.8606507059545736,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002914169941060903,
      "loss": 0.4053,
      "step": 5825
    },
    {
      "epoch": 2.8631062001227745,
      "grad_norm": 1.4921875,
      "learning_rate": 0.000291409626719057,
      "loss": 0.4602,
      "step": 5830
    },
    {
      "epoch": 2.8655616942909763,
      "grad_norm": 1.671875,
      "learning_rate": 0.00029140225933202353,
      "loss": 0.4235,
      "step": 5835
    },
    {
      "epoch": 2.868017188459177,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029139489194499014,
      "loss": 0.424,
      "step": 5840
    },
    {
      "epoch": 2.870472682627379,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029138752455795674,
      "loss": 0.4134,
      "step": 5845
    },
    {
      "epoch": 2.87292817679558,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029138015717092335,
      "loss": 0.4189,
      "step": 5850
    },
    {
      "epoch": 2.8753836709637817,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029137278978388995,
      "loss": 0.4193,
      "step": 5855
    },
    {
      "epoch": 2.8778391651319826,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00029136542239685656,
      "loss": 0.4249,
      "step": 5860
    },
    {
      "epoch": 2.8802946593001844,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029135805500982316,
      "loss": 0.4142,
      "step": 5865
    },
    {
      "epoch": 2.8827501534683853,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029135068762278977,
      "loss": 0.4112,
      "step": 5870
    },
    {
      "epoch": 2.885205647636587,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002913433202357564,
      "loss": 0.4552,
      "step": 5875
    },
    {
      "epoch": 2.887661141804788,
      "grad_norm": 1.59375,
      "learning_rate": 0.000291335952848723,
      "loss": 0.4415,
      "step": 5880
    },
    {
      "epoch": 2.8901166359729897,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00029132858546168953,
      "loss": 0.4365,
      "step": 5885
    },
    {
      "epoch": 2.892572130141191,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002913212180746562,
      "loss": 0.4225,
      "step": 5890
    },
    {
      "epoch": 2.8950276243093924,
      "grad_norm": 1.484375,
      "learning_rate": 0.00029131385068762274,
      "loss": 0.3957,
      "step": 5895
    },
    {
      "epoch": 2.8974831184775938,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029130648330058935,
      "loss": 0.4298,
      "step": 5900
    },
    {
      "epoch": 2.899938612645795,
      "grad_norm": 1.625,
      "learning_rate": 0.000291299115913556,
      "loss": 0.4197,
      "step": 5905
    },
    {
      "epoch": 2.9023941068139965,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029129174852652256,
      "loss": 0.4196,
      "step": 5910
    },
    {
      "epoch": 2.904849600982198,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029128438113948916,
      "loss": 0.4123,
      "step": 5915
    },
    {
      "epoch": 2.907305095150399,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029127701375245577,
      "loss": 0.4175,
      "step": 5920
    },
    {
      "epoch": 2.9097605893186005,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002912696463654224,
      "loss": 0.4216,
      "step": 5925
    },
    {
      "epoch": 2.912216083486802,
      "grad_norm": 1.5078125,
      "learning_rate": 0.000291262278978389,
      "loss": 0.4502,
      "step": 5930
    },
    {
      "epoch": 2.914671577655003,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002912549115913556,
      "loss": 0.4005,
      "step": 5935
    },
    {
      "epoch": 2.9171270718232045,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029124754420432213,
      "loss": 0.4226,
      "step": 5940
    },
    {
      "epoch": 2.919582565991406,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002912401768172888,
      "loss": 0.4464,
      "step": 5945
    },
    {
      "epoch": 2.922038060159607,
      "grad_norm": 1.6875,
      "learning_rate": 0.0002912328094302554,
      "loss": 0.4354,
      "step": 5950
    },
    {
      "epoch": 2.9244935543278086,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029122544204322195,
      "loss": 0.4239,
      "step": 5955
    },
    {
      "epoch": 2.92694904849601,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002912180746561886,
      "loss": 0.401,
      "step": 5960
    },
    {
      "epoch": 2.9294045426642112,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029121070726915516,
      "loss": 0.398,
      "step": 5965
    },
    {
      "epoch": 2.9318600368324126,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029120333988212177,
      "loss": 0.3957,
      "step": 5970
    },
    {
      "epoch": 2.934315531000614,
      "grad_norm": 1.546875,
      "learning_rate": 0.00029119597249508837,
      "loss": 0.4196,
      "step": 5975
    },
    {
      "epoch": 2.9367710251688153,
      "grad_norm": 1.6328125,
      "learning_rate": 0.000291188605108055,
      "loss": 0.4119,
      "step": 5980
    },
    {
      "epoch": 2.9392265193370166,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002911812377210216,
      "loss": 0.401,
      "step": 5985
    },
    {
      "epoch": 2.941682013505218,
      "grad_norm": 1.5,
      "learning_rate": 0.0002911738703339882,
      "loss": 0.4019,
      "step": 5990
    },
    {
      "epoch": 2.9441375076734193,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002911665029469548,
      "loss": 0.4308,
      "step": 5995
    },
    {
      "epoch": 2.9465930018416207,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002911591355599214,
      "loss": 0.3977,
      "step": 6000
    },
    {
      "epoch": 2.949048496009822,
      "grad_norm": 1.609375,
      "learning_rate": 0.000291151768172888,
      "loss": 0.3724,
      "step": 6005
    },
    {
      "epoch": 2.9515039901780233,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002911444007858546,
      "loss": 0.4056,
      "step": 6010
    },
    {
      "epoch": 2.9539594843462247,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002911370333988212,
      "loss": 0.4419,
      "step": 6015
    },
    {
      "epoch": 2.956414978514426,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002911296660117878,
      "loss": 0.4139,
      "step": 6020
    },
    {
      "epoch": 2.9588704726826274,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029112229862475437,
      "loss": 0.4455,
      "step": 6025
    },
    {
      "epoch": 2.9613259668508287,
      "grad_norm": 1.5,
      "learning_rate": 0.000291114931237721,
      "loss": 0.4134,
      "step": 6030
    },
    {
      "epoch": 2.96378146101903,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029110756385068763,
      "loss": 0.4155,
      "step": 6035
    },
    {
      "epoch": 2.9662369551872314,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002911001964636542,
      "loss": 0.4148,
      "step": 6040
    },
    {
      "epoch": 2.9686924493554327,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002910928290766208,
      "loss": 0.4369,
      "step": 6045
    },
    {
      "epoch": 2.971147943523634,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002910854616895874,
      "loss": 0.4414,
      "step": 6050
    },
    {
      "epoch": 2.9736034376918354,
      "grad_norm": 1.6484375,
      "learning_rate": 0.000291078094302554,
      "loss": 0.3789,
      "step": 6055
    },
    {
      "epoch": 2.976058931860037,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002910707269155206,
      "loss": 0.4184,
      "step": 6060
    },
    {
      "epoch": 2.978514426028238,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002910633595284872,
      "loss": 0.4229,
      "step": 6065
    },
    {
      "epoch": 2.9809699201964395,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002910559921414538,
      "loss": 0.4444,
      "step": 6070
    },
    {
      "epoch": 2.983425414364641,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002910486247544204,
      "loss": 0.4164,
      "step": 6075
    },
    {
      "epoch": 2.985880908532842,
      "grad_norm": 1.609375,
      "learning_rate": 0.000291041257367387,
      "loss": 0.4366,
      "step": 6080
    },
    {
      "epoch": 2.9883364027010435,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002910338899803536,
      "loss": 0.4061,
      "step": 6085
    },
    {
      "epoch": 2.990791896869245,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029102652259332024,
      "loss": 0.4266,
      "step": 6090
    },
    {
      "epoch": 2.993247391037446,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002910191552062868,
      "loss": 0.4265,
      "step": 6095
    },
    {
      "epoch": 2.9957028852056475,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002910117878192534,
      "loss": 0.4037,
      "step": 6100
    },
    {
      "epoch": 2.998158379373849,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00029100442043222,
      "loss": 0.4149,
      "step": 6105
    },
    {
      "epoch": 3.0006138735420502,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0002909970530451866,
      "loss": 0.3861,
      "step": 6110
    },
    {
      "epoch": 3.0030693677102516,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002909896856581532,
      "loss": 0.374,
      "step": 6115
    },
    {
      "epoch": 3.005524861878453,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002909823182711198,
      "loss": 0.3667,
      "step": 6120
    },
    {
      "epoch": 3.0079803560466543,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002909749508840864,
      "loss": 0.3675,
      "step": 6125
    },
    {
      "epoch": 3.0104358502148556,
      "grad_norm": 1.640625,
      "learning_rate": 0.000290967583497053,
      "loss": 0.4036,
      "step": 6130
    },
    {
      "epoch": 3.012891344383057,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029096021611001963,
      "loss": 0.3704,
      "step": 6135
    },
    {
      "epoch": 3.0153468385512583,
      "grad_norm": 1.703125,
      "learning_rate": 0.00029095284872298623,
      "loss": 0.3954,
      "step": 6140
    },
    {
      "epoch": 3.0178023327194596,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029094548133595284,
      "loss": 0.399,
      "step": 6145
    },
    {
      "epoch": 3.020257826887661,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029093811394891944,
      "loss": 0.3889,
      "step": 6150
    },
    {
      "epoch": 3.0227133210558623,
      "grad_norm": 1.5078125,
      "learning_rate": 0.000290930746561886,
      "loss": 0.3593,
      "step": 6155
    },
    {
      "epoch": 3.0251688152240637,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002909233791748526,
      "loss": 0.3781,
      "step": 6160
    },
    {
      "epoch": 3.027624309392265,
      "grad_norm": 1.5,
      "learning_rate": 0.00029091601178781926,
      "loss": 0.3974,
      "step": 6165
    },
    {
      "epoch": 3.0300798035604664,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002909086444007858,
      "loss": 0.3751,
      "step": 6170
    },
    {
      "epoch": 3.0325352977286677,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002909012770137524,
      "loss": 0.3529,
      "step": 6175
    },
    {
      "epoch": 3.034990791896869,
      "grad_norm": 1.6015625,
      "learning_rate": 0.000290893909626719,
      "loss": 0.3884,
      "step": 6180
    },
    {
      "epoch": 3.0374462860650704,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002908865422396856,
      "loss": 0.4006,
      "step": 6185
    },
    {
      "epoch": 3.0399017802332717,
      "grad_norm": 1.515625,
      "learning_rate": 0.00029087917485265223,
      "loss": 0.3779,
      "step": 6190
    },
    {
      "epoch": 3.042357274401473,
      "grad_norm": 1.5,
      "learning_rate": 0.00029087180746561884,
      "loss": 0.3772,
      "step": 6195
    },
    {
      "epoch": 3.044812768569675,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00029086444007858544,
      "loss": 0.3718,
      "step": 6200
    },
    {
      "epoch": 3.047268262737876,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029085707269155205,
      "loss": 0.3769,
      "step": 6205
    },
    {
      "epoch": 3.0497237569060776,
      "grad_norm": 1.375,
      "learning_rate": 0.00029084970530451865,
      "loss": 0.3805,
      "step": 6210
    },
    {
      "epoch": 3.052179251074279,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002908423379174852,
      "loss": 0.3757,
      "step": 6215
    },
    {
      "epoch": 3.0546347452424802,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029083497053045186,
      "loss": 0.3788,
      "step": 6220
    },
    {
      "epoch": 3.0570902394106816,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029082760314341847,
      "loss": 0.3861,
      "step": 6225
    },
    {
      "epoch": 3.059545733578883,
      "grad_norm": 1.5625,
      "learning_rate": 0.000290820235756385,
      "loss": 0.4034,
      "step": 6230
    },
    {
      "epoch": 3.0620012277470843,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002908128683693517,
      "loss": 0.3861,
      "step": 6235
    },
    {
      "epoch": 3.0644567219152856,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029080550098231823,
      "loss": 0.4125,
      "step": 6240
    },
    {
      "epoch": 3.066912216083487,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029079813359528484,
      "loss": 0.3785,
      "step": 6245
    },
    {
      "epoch": 3.0693677102516883,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029079076620825144,
      "loss": 0.3654,
      "step": 6250
    },
    {
      "epoch": 3.0718232044198897,
      "grad_norm": 1.609375,
      "learning_rate": 0.00029078339882121805,
      "loss": 0.3867,
      "step": 6255
    },
    {
      "epoch": 3.074278698588091,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029077603143418465,
      "loss": 0.3649,
      "step": 6260
    },
    {
      "epoch": 3.0767341927562923,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029076866404715126,
      "loss": 0.3902,
      "step": 6265
    },
    {
      "epoch": 3.0791896869244937,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029076129666011786,
      "loss": 0.4011,
      "step": 6270
    },
    {
      "epoch": 3.081645181092695,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029075392927308447,
      "loss": 0.3756,
      "step": 6275
    },
    {
      "epoch": 3.0841006752608964,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029074656188605107,
      "loss": 0.3752,
      "step": 6280
    },
    {
      "epoch": 3.0865561694290977,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002907391944990176,
      "loss": 0.3912,
      "step": 6285
    },
    {
      "epoch": 3.089011663597299,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002907318271119843,
      "loss": 0.3548,
      "step": 6290
    },
    {
      "epoch": 3.0914671577655004,
      "grad_norm": 1.4375,
      "learning_rate": 0.0002907244597249509,
      "loss": 0.3777,
      "step": 6295
    },
    {
      "epoch": 3.0939226519337018,
      "grad_norm": 1.453125,
      "learning_rate": 0.00029071709233791744,
      "loss": 0.3784,
      "step": 6300
    },
    {
      "epoch": 3.096378146101903,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029070972495088404,
      "loss": 0.3829,
      "step": 6305
    },
    {
      "epoch": 3.0988336402701044,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029070235756385065,
      "loss": 0.3758,
      "step": 6310
    },
    {
      "epoch": 3.101289134438306,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029069499017681725,
      "loss": 0.3783,
      "step": 6315
    },
    {
      "epoch": 3.103744628606507,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029068762278978386,
      "loss": 0.3783,
      "step": 6320
    },
    {
      "epoch": 3.1062001227747085,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00029068025540275046,
      "loss": 0.3747,
      "step": 6325
    },
    {
      "epoch": 3.10865561694291,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029067288801571707,
      "loss": 0.3718,
      "step": 6330
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 1.6875,
      "learning_rate": 0.0002906655206286837,
      "loss": 0.395,
      "step": 6335
    },
    {
      "epoch": 3.1135666052793125,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002906581532416503,
      "loss": 0.3947,
      "step": 6340
    },
    {
      "epoch": 3.116022099447514,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002906507858546169,
      "loss": 0.3853,
      "step": 6345
    },
    {
      "epoch": 3.118477593615715,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002906434184675835,
      "loss": 0.3664,
      "step": 6350
    },
    {
      "epoch": 3.1209330877839165,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002906360510805501,
      "loss": 0.4038,
      "step": 6355
    },
    {
      "epoch": 3.123388581952118,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029062868369351665,
      "loss": 0.3697,
      "step": 6360
    },
    {
      "epoch": 3.1258440761203192,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002906213163064833,
      "loss": 0.3832,
      "step": 6365
    },
    {
      "epoch": 3.1282995702885206,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029061394891944986,
      "loss": 0.3789,
      "step": 6370
    },
    {
      "epoch": 3.130755064456722,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00029060658153241646,
      "loss": 0.4075,
      "step": 6375
    },
    {
      "epoch": 3.1332105586249233,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029059921414538307,
      "loss": 0.3795,
      "step": 6380
    },
    {
      "epoch": 3.1356660527931246,
      "grad_norm": 1.625,
      "learning_rate": 0.00029059184675834967,
      "loss": 0.3849,
      "step": 6385
    },
    {
      "epoch": 3.138121546961326,
      "grad_norm": 1.640625,
      "learning_rate": 0.0002905844793713163,
      "loss": 0.367,
      "step": 6390
    },
    {
      "epoch": 3.1405770411295273,
      "grad_norm": 1.703125,
      "learning_rate": 0.0002905771119842829,
      "loss": 0.3739,
      "step": 6395
    },
    {
      "epoch": 3.1430325352977286,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002905697445972495,
      "loss": 0.3899,
      "step": 6400
    },
    {
      "epoch": 3.14548802946593,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002905623772102161,
      "loss": 0.4,
      "step": 6405
    },
    {
      "epoch": 3.1479435236341313,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0002905550098231827,
      "loss": 0.3864,
      "step": 6410
    },
    {
      "epoch": 3.1503990178023327,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029054764243614925,
      "loss": 0.3806,
      "step": 6415
    },
    {
      "epoch": 3.152854511970534,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002905402750491159,
      "loss": 0.3988,
      "step": 6420
    },
    {
      "epoch": 3.1553100061387354,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002905329076620825,
      "loss": 0.3949,
      "step": 6425
    },
    {
      "epoch": 3.1577655003069367,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029052554027504907,
      "loss": 0.3798,
      "step": 6430
    },
    {
      "epoch": 3.160220994475138,
      "grad_norm": 1.4375,
      "learning_rate": 0.00029051817288801567,
      "loss": 0.3535,
      "step": 6435
    },
    {
      "epoch": 3.1626764886433394,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002905108055009823,
      "loss": 0.3895,
      "step": 6440
    },
    {
      "epoch": 3.1651319828115407,
      "grad_norm": 1.40625,
      "learning_rate": 0.0002905034381139489,
      "loss": 0.3628,
      "step": 6445
    },
    {
      "epoch": 3.167587476979742,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002904960707269155,
      "loss": 0.3907,
      "step": 6450
    },
    {
      "epoch": 3.1700429711479434,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002904887033398821,
      "loss": 0.3834,
      "step": 6455
    },
    {
      "epoch": 3.1724984653161448,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002904813359528487,
      "loss": 0.3722,
      "step": 6460
    },
    {
      "epoch": 3.174953959484346,
      "grad_norm": 1.5,
      "learning_rate": 0.0002904739685658153,
      "loss": 0.4012,
      "step": 6465
    },
    {
      "epoch": 3.1774094536525475,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002904666011787819,
      "loss": 0.3781,
      "step": 6470
    },
    {
      "epoch": 3.179864947820749,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002904592337917485,
      "loss": 0.3984,
      "step": 6475
    },
    {
      "epoch": 3.18232044198895,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002904518664047151,
      "loss": 0.3951,
      "step": 6480
    },
    {
      "epoch": 3.1847759361571515,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002904444990176817,
      "loss": 0.3788,
      "step": 6485
    },
    {
      "epoch": 3.187231430325353,
      "grad_norm": 1.5859375,
      "learning_rate": 0.0002904371316306483,
      "loss": 0.3979,
      "step": 6490
    },
    {
      "epoch": 3.189686924493554,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029042976424361493,
      "loss": 0.3824,
      "step": 6495
    },
    {
      "epoch": 3.1921424186617555,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0002904223968565815,
      "loss": 0.3987,
      "step": 6500
    },
    {
      "epoch": 3.194597912829957,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0002904150294695481,
      "loss": 0.3963,
      "step": 6505
    },
    {
      "epoch": 3.197053406998158,
      "grad_norm": 1.4140625,
      "learning_rate": 0.00029040766208251475,
      "loss": 0.3983,
      "step": 6510
    },
    {
      "epoch": 3.1995089011663596,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002904002946954813,
      "loss": 0.3891,
      "step": 6515
    },
    {
      "epoch": 3.201964395334561,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002903929273084479,
      "loss": 0.3668,
      "step": 6520
    },
    {
      "epoch": 3.2044198895027622,
      "grad_norm": 1.625,
      "learning_rate": 0.0002903855599214145,
      "loss": 0.3883,
      "step": 6525
    },
    {
      "epoch": 3.2068753836709636,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002903781925343811,
      "loss": 0.3906,
      "step": 6530
    },
    {
      "epoch": 3.209330877839165,
      "grad_norm": 1.734375,
      "learning_rate": 0.0002903708251473477,
      "loss": 0.3779,
      "step": 6535
    },
    {
      "epoch": 3.2117863720073663,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002903634577603143,
      "loss": 0.4081,
      "step": 6540
    },
    {
      "epoch": 3.214241866175568,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002903560903732809,
      "loss": 0.3918,
      "step": 6545
    },
    {
      "epoch": 3.216697360343769,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029034872298624754,
      "loss": 0.3775,
      "step": 6550
    },
    {
      "epoch": 3.2191528545119708,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00029034135559921414,
      "loss": 0.4307,
      "step": 6555
    },
    {
      "epoch": 3.2216083486801717,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002903339882121807,
      "loss": 0.3895,
      "step": 6560
    },
    {
      "epoch": 3.2240638428483734,
      "grad_norm": 1.59375,
      "learning_rate": 0.00029032662082514735,
      "loss": 0.3781,
      "step": 6565
    },
    {
      "epoch": 3.226519337016575,
      "grad_norm": 1.6875,
      "learning_rate": 0.00029031925343811396,
      "loss": 0.3797,
      "step": 6570
    },
    {
      "epoch": 3.228974831184776,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002903118860510805,
      "loss": 0.3758,
      "step": 6575
    },
    {
      "epoch": 3.2314303253529775,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002903045186640471,
      "loss": 0.3915,
      "step": 6580
    },
    {
      "epoch": 3.233885819521179,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002902971512770137,
      "loss": 0.3933,
      "step": 6585
    },
    {
      "epoch": 3.23634131368938,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0002902897838899803,
      "loss": 0.3813,
      "step": 6590
    },
    {
      "epoch": 3.2387968078575815,
      "grad_norm": 1.3359375,
      "learning_rate": 0.00029028241650294693,
      "loss": 0.3891,
      "step": 6595
    },
    {
      "epoch": 3.241252302025783,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00029027504911591353,
      "loss": 0.3892,
      "step": 6600
    },
    {
      "epoch": 3.243707796193984,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029026768172888014,
      "loss": 0.3896,
      "step": 6605
    },
    {
      "epoch": 3.2461632903621855,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00029026031434184674,
      "loss": 0.3846,
      "step": 6610
    },
    {
      "epoch": 3.248618784530387,
      "grad_norm": 1.5625,
      "learning_rate": 0.00029025294695481335,
      "loss": 0.4087,
      "step": 6615
    },
    {
      "epoch": 3.2510742786985882,
      "grad_norm": 1.421875,
      "learning_rate": 0.00029024557956777995,
      "loss": 0.3795,
      "step": 6620
    },
    {
      "epoch": 3.2535297728667896,
      "grad_norm": 1.640625,
      "learning_rate": 0.00029023821218074656,
      "loss": 0.3833,
      "step": 6625
    },
    {
      "epoch": 3.255985267034991,
      "grad_norm": 1.6875,
      "learning_rate": 0.0002902308447937131,
      "loss": 0.364,
      "step": 6630
    },
    {
      "epoch": 3.2584407612031923,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002902234774066797,
      "loss": 0.3825,
      "step": 6635
    },
    {
      "epoch": 3.2608962553713936,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002902161100196464,
      "loss": 0.4021,
      "step": 6640
    },
    {
      "epoch": 3.263351749539595,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0002902087426326129,
      "loss": 0.3964,
      "step": 6645
    },
    {
      "epoch": 3.2658072437077963,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029020137524557953,
      "loss": 0.3904,
      "step": 6650
    },
    {
      "epoch": 3.2682627378759976,
      "grad_norm": 1.46875,
      "learning_rate": 0.00029019400785854614,
      "loss": 0.4019,
      "step": 6655
    },
    {
      "epoch": 3.270718232044199,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029018664047151274,
      "loss": 0.3477,
      "step": 6660
    },
    {
      "epoch": 3.2731737262124003,
      "grad_norm": 1.671875,
      "learning_rate": 0.00029017927308447935,
      "loss": 0.3923,
      "step": 6665
    },
    {
      "epoch": 3.2756292203806017,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029017190569744595,
      "loss": 0.3666,
      "step": 6670
    },
    {
      "epoch": 3.278084714548803,
      "grad_norm": 1.6328125,
      "learning_rate": 0.00029016453831041256,
      "loss": 0.403,
      "step": 6675
    },
    {
      "epoch": 3.2805402087170044,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00029015717092337916,
      "loss": 0.3694,
      "step": 6680
    },
    {
      "epoch": 3.2829957028852057,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00029014980353634577,
      "loss": 0.3899,
      "step": 6685
    },
    {
      "epoch": 3.285451197053407,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0002901424361493123,
      "loss": 0.3944,
      "step": 6690
    },
    {
      "epoch": 3.2879066912216084,
      "grad_norm": 1.6015625,
      "learning_rate": 0.000290135068762279,
      "loss": 0.3908,
      "step": 6695
    },
    {
      "epoch": 3.2903621853898097,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002901277013752456,
      "loss": 0.3694,
      "step": 6700
    },
    {
      "epoch": 3.292817679558011,
      "grad_norm": 1.625,
      "learning_rate": 0.00029012033398821213,
      "loss": 0.382,
      "step": 6705
    },
    {
      "epoch": 3.2952731737262124,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029011296660117874,
      "loss": 0.3883,
      "step": 6710
    },
    {
      "epoch": 3.2977286678944138,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029010559921414535,
      "loss": 0.3657,
      "step": 6715
    },
    {
      "epoch": 3.300184162062615,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00029009823182711195,
      "loss": 0.3792,
      "step": 6720
    },
    {
      "epoch": 3.3026396562308165,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00029009086444007856,
      "loss": 0.3814,
      "step": 6725
    },
    {
      "epoch": 3.305095150399018,
      "grad_norm": 1.671875,
      "learning_rate": 0.00029008349705304516,
      "loss": 0.3876,
      "step": 6730
    },
    {
      "epoch": 3.307550644567219,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00029007612966601177,
      "loss": 0.3632,
      "step": 6735
    },
    {
      "epoch": 3.3100061387354205,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00029006876227897837,
      "loss": 0.3694,
      "step": 6740
    },
    {
      "epoch": 3.312461632903622,
      "grad_norm": 1.5234375,
      "learning_rate": 0.000290061394891945,
      "loss": 0.4084,
      "step": 6745
    },
    {
      "epoch": 3.314917127071823,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002900540275049116,
      "loss": 0.3931,
      "step": 6750
    },
    {
      "epoch": 3.3173726212400245,
      "grad_norm": 1.3671875,
      "learning_rate": 0.0002900466601178782,
      "loss": 0.3543,
      "step": 6755
    },
    {
      "epoch": 3.319828115408226,
      "grad_norm": 1.53125,
      "learning_rate": 0.00029003929273084474,
      "loss": 0.391,
      "step": 6760
    },
    {
      "epoch": 3.322283609576427,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00029003192534381134,
      "loss": 0.3976,
      "step": 6765
    },
    {
      "epoch": 3.3247391037446286,
      "grad_norm": 1.5390625,
      "learning_rate": 0.000290024557956778,
      "loss": 0.3613,
      "step": 6770
    },
    {
      "epoch": 3.32719459791283,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00029001719056974455,
      "loss": 0.3583,
      "step": 6775
    },
    {
      "epoch": 3.3296500920810312,
      "grad_norm": 1.625,
      "learning_rate": 0.00029000982318271116,
      "loss": 0.3776,
      "step": 6780
    },
    {
      "epoch": 3.3321055862492326,
      "grad_norm": 1.578125,
      "learning_rate": 0.00029000245579567776,
      "loss": 0.3879,
      "step": 6785
    },
    {
      "epoch": 3.334561080417434,
      "grad_norm": 1.515625,
      "learning_rate": 0.00028999508840864437,
      "loss": 0.3893,
      "step": 6790
    },
    {
      "epoch": 3.3370165745856353,
      "grad_norm": 1.4296875,
      "learning_rate": 0.000289987721021611,
      "loss": 0.3709,
      "step": 6795
    },
    {
      "epoch": 3.3394720687538366,
      "grad_norm": 1.546875,
      "learning_rate": 0.0002899803536345776,
      "loss": 0.3558,
      "step": 6800
    },
    {
      "epoch": 3.341927562922038,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0002899729862475442,
      "loss": 0.4007,
      "step": 6805
    },
    {
      "epoch": 3.3443830570902393,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002899656188605108,
      "loss": 0.3805,
      "step": 6810
    },
    {
      "epoch": 3.3468385512584407,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002899582514734774,
      "loss": 0.4006,
      "step": 6815
    },
    {
      "epoch": 3.349294045426642,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00028995088408644395,
      "loss": 0.3768,
      "step": 6820
    },
    {
      "epoch": 3.3517495395948433,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0002899435166994106,
      "loss": 0.385,
      "step": 6825
    },
    {
      "epoch": 3.3542050337630447,
      "grad_norm": 1.765625,
      "learning_rate": 0.0002899361493123772,
      "loss": 0.3894,
      "step": 6830
    },
    {
      "epoch": 3.356660527931246,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00028992878192534376,
      "loss": 0.4179,
      "step": 6835
    },
    {
      "epoch": 3.3591160220994474,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002899214145383104,
      "loss": 0.3489,
      "step": 6840
    },
    {
      "epoch": 3.3615715162676487,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00028991404715127697,
      "loss": 0.4076,
      "step": 6845
    },
    {
      "epoch": 3.36402701043585,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002899066797642436,
      "loss": 0.3609,
      "step": 6850
    },
    {
      "epoch": 3.3664825046040514,
      "grad_norm": 1.7734375,
      "learning_rate": 0.0002898993123772102,
      "loss": 0.3675,
      "step": 6855
    },
    {
      "epoch": 3.3689379987722528,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0002898919449901768,
      "loss": 0.3947,
      "step": 6860
    },
    {
      "epoch": 3.371393492940454,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002898845776031434,
      "loss": 0.372,
      "step": 6865
    },
    {
      "epoch": 3.3738489871086554,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00028987721021611,
      "loss": 0.3598,
      "step": 6870
    },
    {
      "epoch": 3.376304481276857,
      "grad_norm": 1.625,
      "learning_rate": 0.0002898698428290766,
      "loss": 0.3844,
      "step": 6875
    },
    {
      "epoch": 3.378759975445058,
      "grad_norm": 1.609375,
      "learning_rate": 0.0002898624754420432,
      "loss": 0.3696,
      "step": 6880
    },
    {
      "epoch": 3.3812154696132595,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0002898551080550098,
      "loss": 0.3793,
      "step": 6885
    },
    {
      "epoch": 3.3836709637814613,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00028984774066797636,
      "loss": 0.3573,
      "step": 6890
    },
    {
      "epoch": 3.386126457949662,
      "grad_norm": 1.671875,
      "learning_rate": 0.00028984037328094297,
      "loss": 0.3988,
      "step": 6895
    },
    {
      "epoch": 3.388581952117864,
      "grad_norm": 1.5,
      "learning_rate": 0.00028983300589390963,
      "loss": 0.3844,
      "step": 6900
    },
    {
      "epoch": 3.391037446286065,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0002898256385068762,
      "loss": 0.4001,
      "step": 6905
    },
    {
      "epoch": 3.3934929404542666,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0002898182711198428,
      "loss": 0.3539,
      "step": 6910
    },
    {
      "epoch": 3.3959484346224675,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00028981090373280944,
      "loss": 0.3855,
      "step": 6915
    },
    {
      "epoch": 3.3984039287906693,
      "grad_norm": 1.6171875,
      "learning_rate": 0.000289803536345776,
      "loss": 0.3819,
      "step": 6920
    },
    {
      "epoch": 3.4008594229588702,
      "grad_norm": 1.65625,
      "learning_rate": 0.0002897961689587426,
      "loss": 0.3817,
      "step": 6925
    },
    {
      "epoch": 3.403314917127072,
      "grad_norm": 1.59375,
      "learning_rate": 0.0002897888015717092,
      "loss": 0.3821,
      "step": 6930
    },
    {
      "epoch": 3.4057704112952734,
      "grad_norm": 1.6953125,
      "learning_rate": 0.0002897814341846758,
      "loss": 0.3694,
      "step": 6935
    },
    {
      "epoch": 3.4082259054634747,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0002897740667976424,
      "loss": 0.3562,
      "step": 6940
    },
    {
      "epoch": 3.410681399631676,
      "grad_norm": 1.40625,
      "learning_rate": 0.000289766699410609,
      "loss": 0.3804,
      "step": 6945
    },
    {
      "epoch": 3.4131368937998774,
      "grad_norm": 1.53125,
      "learning_rate": 0.0002897593320235756,
      "loss": 0.3994,
      "step": 6950
    },
    {
      "epoch": 3.4155923879680787,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00028975196463654223,
      "loss": 0.3821,
      "step": 6955
    },
    {
      "epoch": 3.41804788213628,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00028974459724950884,
      "loss": 0.3933,
      "step": 6960
    },
    {
      "epoch": 3.4205033763044814,
      "grad_norm": 1.7265625,
      "learning_rate": 0.0002897372298624754,
      "loss": 0.3737,
      "step": 6965
    },
    {
      "epoch": 3.4229588704726828,
      "grad_norm": 1.6328125,
      "learning_rate": 0.00028972986247544205,
      "loss": 0.4001,
      "step": 6970
    },
    {
      "epoch": 3.425414364640884,
      "grad_norm": 1.625,
      "learning_rate": 0.0002897224950884086,
      "loss": 0.3554,
      "step": 6975
    },
    {
      "epoch": 3.4278698588090855,
      "grad_norm": 1.515625,
      "learning_rate": 0.0002897151277013752,
      "loss": 0.3799,
      "step": 6980
    },
    {
      "epoch": 3.430325352977287,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002897077603143418,
      "loss": 0.3759,
      "step": 6985
    },
    {
      "epoch": 3.432780847145488,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0002897003929273084,
      "loss": 0.3792,
      "step": 6990
    },
    {
      "epoch": 3.4352363413136895,
      "grad_norm": 1.4765625,
      "learning_rate": 0.000289693025540275,
      "loss": 0.3716,
      "step": 6995
    },
    {
      "epoch": 3.437691835481891,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0002896856581532416,
      "loss": 0.3666,
      "step": 7000
    }
  ],
  "logging_steps": 5,
  "max_steps": 203600,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.277060944074465e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
